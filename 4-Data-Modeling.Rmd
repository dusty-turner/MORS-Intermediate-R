# Data Modeling

In the last year or so, [Max Kuhn]("https://twitter.com/topepos"), working for RStudio, has developed `tidymodels`.  Tidymodels is a modeling framework that follows tidy coding principles.  It provides a streamlined technique for preprocessing, execution, and validation 

In this section, you will learn:

1. A small amount of data exploration.
2. The basics of `tidymodels`
 - splitting data
 - running multiple models
 - cross validation
 - selecting the best model
3. How to parallelize your code

In this section, we will use the following libraries and data:

```{r, cache=TRUE}
library(tidyverse)
library(tidymodels)
library(recipes)
library(tune)
library(janitor)
library(doFuture)
library(yardstick)
library(vip)

data <- read_csv("data_sources/Batting.csv", col_types = cols(SF = col_double(), GIDP = col_double())) %>%
  clean_names()

hofdata <- read_csv("data_sources/HallOfFame.csv") 

head(data)

head(hofdata)
```

Lets try to develop an informed answer to the question: What baseball statistics help indicate whether or not a baseball player will make the Hall of Fame?

## Prepare Data For Analysis

For this analysis, lets organize our data to identify players who are in the hall of fame as well as those eligible.

```{r, cache=TRUE}

hofdata <-
  hofdata %>% 
  clean_names() %>% 
  select(player_id, inducted) %>% 
  mutate(hof = ifelse(inducted=="Y",1,0)) %>% 
  filter(hof==1)

hofdata %>%  count(inducted)

years_played <- 
data %>% 
  group_by(player_id) %>% 
  summarise(across(year_id, .fns = c("min" = min, "max" = max))) %>%  # dplyr 1.0.0
  mutate(total_years = year_id_max - year_id_min)

years_played

hof <-
  data %>% 
  group_by(player_id) %>% 
  summarise_at(vars(g:gidp), list(~sum(.,na.rm = TRUE))) %>%
  ungroup() %>%  
  left_join(years_played, by = "player_id") %>% 
  left_join(hofdata, by = "player_id") %>% 
  mutate(inducted = if_else(inducted=="Y",1,0)) %>% 
  mutate(inducted = replace_na(inducted, 0)) %>% 
  filter(total_years >= 10) %>% 
  # filter(g>=1000) %>% 
  mutate(inducted = as.factor(inducted)) %>% 
  select(-hof)

hof

```

### Filter for HOF Eligible Players {-}

```{r, cache=TRUE}
hof %>% 
  filter(year_id_max <= 2012) %>% 
  count(inducted) 

hofmod <-
hof %>% 
  filter(year_id_max <= 2012) %>% 
  select(-contains("year"))
hofmod

hoftest <-
hof %>% 
  filter(year_id_max > 2012) %>% 
  select(-contains("year"),-inducted)
hoftest

```

## Data Exploration

Percentage of players in the dataset who are in the HOF

```{r, cache=TRUE}
hofmod %>% count(inducted) %>% mutate(prop = n/sum(n))
```

Visual difference in statistics between HOF and non HOF

```{r, cache=TRUE}

hof %>% 
  select(g:gidp, inducted) %>%  
  pivot_longer(cols = g:gidp) %>% 
  group_by(inducted,name) %>% 
  summarise(
    lwr_quantile = quantile(value, c(.025)),
    median = quantile(value, c(.5)),
    upper_quantile = quantile(value, c(.975)),
            ) %>% 
  ggplot(aes(x=inducted,y=median)) +
  geom_point() +
  geom_errorbar(aes(ymin = lwr_quantile, ymax = upper_quantile)) +
  theme(legend.position = "none") +
  labs(x= "",y = "", title = "Differences Between HOF and non-HOF") +
  facet_wrap(~name, scales = "free")
```

Well, won't players who are in the HOF had played longer?  Therefore they should have more hits, home runs, etc?

Lets compare the number of games of players in the HOF vs out of the HOF.

```{r, cache=TRUE}
hof %>% 
  select(g, inducted) %>%  
  ggplot(aes(x=g, fill = inducted)) +
  geom_density() +
  labs(x="Games", y = "", fill = "HOF",
       title = "Density of Game / HOF Stats")
```


Lets look at this for every statistic

```{r, cache=TRUE}
hof %>% 
  select(g:gidp, inducted) %>%  
  pivot_longer(cols = g:gidp) %>% 
  ggplot(aes(x=value, fill = inducted)) +
  geom_density() +
  facet_wrap(~name, scales = "free") +
  labs(x="",y = "", fill = "HOF",
       title = "Density of Stats for Players In/Out of HOF")
  
```


## Split Data test/train

To begin modeling, we'll need to split our data into a testing, training, and validation set.

```{r, cache=TRUE}
set.seed(str_length("beatnavy"))
hof_initial_split <- initial_split(hofmod, prop = 0.80)
hof_initial_split
```

## Preprocess Data

A recipe is a description of what steps should be applied to a data set in order to get it ready for data analysis.

`tidymodels` / `recipes` currently offers about 30 'steps'.  More documentation for the `recipes` package is [here](https://recipes.tidymodels.org/reference/index.html)

We first specify the "recipe" or specify what we are trying to model and with what data.

```{r}
recipe(inducted ~ ., data = training(hof_initial_split))
```

From here, we need to specify other preprocessing steps.  We will add these to the recipe.   

Once we complete these steps, we 'prep' the data.

```{r, cache=TRUE}
preprocessing_recipe <-
  recipe(inducted ~ ., data = training(hof_initial_split)) %>%
  step_knnimpute(all_numeric()) %>% 
  step_center(all_numeric()) %>% 
  step_scale(all_numeric()) %>% 
  step_rm(player_id) %>%
  prep()
preprocessing_recipe
```

## Apply Preprocessing

For a recipe with at least one preprocessing operations that has been trained by `recipe()`, apply the computations to the data.

```{r, cache=TRUE}
hof_training_preprocessed_tbl <- 
  preprocessing_recipe %>% 
  bake(training(hof_initial_split))

hof_training_preprocessed_tbl
```

## Prepare Cross Validation

This partitions our data into `v` folds.  In our case, 5.

This yields a data frame with a nested list of training / testing data.

```{r, cache=TRUE}
set.seed(str_length("beatnavy"))
hof_cv_folds <-
  training(hof_initial_split) %>% 
    bake(preprocessing_recipe, new_data = .) %>%
    vfold_cv(v = 5)
hof_cv_folds
```

## Specify Models 

Now that we've prepared our data, we must specify the models which want to compare.  We'll look at 2.

You must first specify the model type (`logistic_reg()` and `rand_forest` in the examples below).  There are many to choose from in the parsnip package.  Documentation can be found [here](tidyverse.org/blog/2018/11/parsnip-0-0-1/).

The motivation behind this package from the documentation, "Modeling functions across different R packages can have very different interfaces. If you would like to try different approaches, there is a lot of syntactical minutiae to remember. The problem worsens when you move in-between platforms.

"`parsnip` tries to solve this by providing similar interfaces to models. For example, if you are fitting a random forest model and would like to adjust the number of trees in the forest there are different argument names to remember depending on the random forest package you chose..." (such as `rf` or `randomforest`).

After you specify the model type, you can provide 

### GLM Model {-}

```{r, cache=TRUE}
glmnet_model <-
  logistic_reg(mode = "classification",
    penalty = tune(),
    mixture = tune()) %>%
  set_engine("glmnet")

glmnet_model
```

### Random Forest Model {-}

Notice how in the previous model and in this model, I specified several tuning parameters.  Other than `penalty`, and `mixture`, there are many to chose from.  These include `mode`, `mtry`, `trees`, and `min_n` among others..  I can specify these values, but I have chose to 'tune' them through cross validation in later steps.

```{r, cache=TRUE}
forest_model <-
  rand_forest(
    mode = "classification",
    mtry = tune(),
    trees = tune(),
    min_n = tune()
  ) %>%
  set_engine("randomForest", objective = "reg:squarederror")

forest_model
```

## Create Grid of Parameters to Validate Over

Here we specify the tuning parameters for cross validation and take a look visually at the parameter space we are covering in efforts to reach the best model. 

### GLM Model {-}

```{r, cache=TRUE}
glmnet_params <- parameters(penalty(), mixture())
glmnet_params

set.seed(str_length("beatnavy"))
glmnet_grid = grid_max_entropy(glmnet_params, size = 20)
glmnet_grid

glmnet_grid %>%
  ggplot(aes(penalty, mixture)) +
  geom_point(size = 3) +
  scale_x_log10() +
  labs(title = "Max Entropy Grid", x = "Penalty (log scale)", y = "Mixture")
```

### Random Forest Model {-}

```{r, cache=TRUE}
forest_params <- parameters(mtry(c(2,6)), trees(), min_n())
forest_params

set.seed(str_length("beatnavy"))
forest_grid <- grid_max_entropy(forest_params, size = 30)
forest_grid

```

## Execute Cross Validation

### Parallel Processing

The great thing about cross validation is that tunes parameters to find the model which performs the best on the out-of-sample test data.  

The problem is that this can be computationally intensive.  

Thankfully `tidymodels` is capable of running over multiple cores.  You can see from the code below my computer has 4 cores.  Your mileage may vary. 

```{r message=FALSE, warning=FALSE, cache=TRUE}
all_cores <- parallel::detectCores(logical = FALSE)
all_cores

registerDoFuture()
cl <- makeCluster(all_cores)
plan(cluster, workers = cl)
```

### GLM Model {-}

The code below kicks off the cross validation of your model.  You can see the time it takes to run the code over multiple cores  

```{r message=FALSE, warning=FALSE, cache=TRUE}

tictoc::tic()
glmnet_stage_1_cv_results_tbl <- tune_grid(
          object = glmnet_model, 
          inducted ~ ., 
          resamples = hof_cv_folds, 
          grid = glmnet_grid, 
          # grid = forest_grid, 
          metrics = metric_set(accuracy, kap, roc_auc), 
          control = control_grid(verbose = TRUE)
          )
tictoc::toc()
```

The output of the cross validation is a tibble with nested columns.  Of note, in the columns are the data and the performance. 

```{r message=FALSE, warning=FALSE, cache=TRUE}
glmnet_stage_1_cv_results_tbl
```

We can manipulate the dataframe to extract the data we need, however, the `tune` package provides a function to help us out.

```{r message=FALSE, warning=FALSE, cache=TRUE}
glmnet_stage_1_cv_results_tbl %>% show_best("accuracy", n = 5) %>% bind_rows(
glmnet_stage_1_cv_results_tbl %>% show_best("kap", n = 5)) %>% bind_rows(
glmnet_stage_1_cv_results_tbl %>% show_best("roc_auc", n = 5))
```

### Random Forest Model {-}

This following code took quite a bit of time - but would have taken about 5 times as long if not parallelized.  

Since we explained the code above, we will not break up this code below.

```{r message=FALSE, warning=FALSE, cache=TRUE}

tictoc::tic()
forest_stage_1_cv_results_tbl <- tune_grid(
    formula   = inducted ~ .,
    model     = forest_model,
    resamples = hof_cv_folds,
    grid      = forest_grid,
    metrics   = metric_set(accuracy, kap, roc_auc),
    control   = control_grid(verbose = TRUE)
)
tictoc::toc()

forest_stage_1_cv_results_tbl 

forest_stage_1_cv_results_tbl %>% show_best("accuracy", n = 5) %>% bind_rows(
forest_stage_1_cv_results_tbl %>% show_best("kap", n = 5)) %>% bind_rows(
forest_stage_1_cv_results_tbl %>% show_best("roc_auc", n = 5))

```

## Select Best Parameters

Now that both models are cross validated, we can select the tuning parameters which minimized our error metric.  

```{r, cache=TRUE}
params_glmnet_best <- 
  glmnet_stage_1_cv_results_tbl %>% 
  select_best("roc_auc")

params_glmnet_best

params_forest_best <-
  forest_stage_1_cv_results_tbl %>% 
  select_best("roc_auc")

params_forest_best
```

## Execute Models With Best Parameters

```{r, cache=TRUE}
glmnet_stage_2_model <-
  glmnet_model %>% 
  finalize_model(parameters = params_glmnet_best)

glmnet_stage_2_model
```

```{r, cache=TRUE}
forest_stage_2_model <-
  forest_model %>% 
  finalize_model(params_forest_best)

forest_stage_2_model
```

## Compare Models

Now we need to compare the 'winning' specifications from our two models to see which one performs best on our error metric.

First we apply our preprocessing recipe to our training and test sets.

```{r, cache=TRUE}
train_processed <-
  training(hof_initial_split) %>% 
  bake(preprocessing_recipe, new_data = .)

test_processed <-
  testing(hof_initial_split) %>% 
  bake(preprocessing_recipe, new_data = .)
```

Next, we are identifying the variable in which we are predicting (inducted in this instance), then using the `metrics` function from the `yardstick` library to estimate the performance of each model.

```{r, cache=TRUE}

target_expr <-
  preprocessing_recipe %>% 
    pluck("last_term_info") %>%
    filter(role == "outcome") %>%
    pull(variable) %>%
    sym()

glmnet_stage_2_metrics <-
  glmnet_stage_2_model %>%
  fit(formula = inducted ~ ., data = train_processed) %>%
  predict(new_data = test_processed) %>%
  bind_cols(testing(hof_initial_split)) %>%
  metrics(!! target_expr, .pred_class)

forest_stage_2_metrics <-   
  forest_stage_2_model %>%
  fit(formula = inducted ~ ., data = train_processed) %>%
  predict(new_data = test_processed) %>%
  bind_cols(testing(hof_initial_split)) %>%
  metrics(!! target_expr, .pred_class)
```

Here, we pretty up the code for easy consumption.

```{r, cache=TRUE}
glmnet_stage_2_metrics %>% 
  mutate(mod = "glmnet") %>% 
  bind_rows(
    forest_stage_2_metrics %>% 
      mutate(mod = "forest")
    ) %>% 
  arrange(.metric,-.estimate)

```

Looks like the Random Forest is the better model.

## Run Best Model on All Data

```{r, cache=TRUE}
model_final <-forest_stage_2_model %>%
    fit(inducted ~ . , data = bake(preprocessing_recipe, new_data = hofmod))
```

## Run Model on New Data

```{r echo=TRUE, include=FALSE, cache=TRUE}
hoftest %>%
  bake(preprocessing_recipe, new_data = .) %>%
  predict(model_final, new_data = .) %>% 
  bind_cols(hoftest %>% select(player_id)) %>% 
  arrange(desc(.pred_class))
```

## Variable Importance

```{r, cache=TRUE}
vip(model_final) +
    labs(title = "Random Forest Model Importance - HOF Prediction") 
```


[Many Thanks](https://www.r-bloggers.com/product-price-prediction-a-tidy-hyperparameter-tuning-and-cross-validation-tutorial/)

