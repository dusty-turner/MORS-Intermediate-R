[
["6-text-analysis.html", "6 Text Analysis 6.1 The Adventures of Tom Sawyer 6.2 Find Chapter Splits 6.3 Tokenize the Book 6.4 Remove ‘stop words’ 6.5 Join Sentiments 6.6 Descriptive Text Statistics 6.7 Visualizations 6.8 N-Gram Analysis 6.9 Term Frequency 6.10 Topic Modeling", " 6 Text Analysis 6.1 The Adventures of Tom Sawyer library(tidyverse) library(tidytext) library(stringi) book = read_file(&quot;data_files//The-Adventures-of-Tom-Sawyer.txt&quot;) %&gt;% enframe(name = &quot;Book&quot;) # book = read_file(&quot;_bookdown_files/data_files/The-Adventures-of-Tom-Sawyer.txt&quot;) %&gt;% enframe(name = &quot;Book&quot;) book ## # A tibble: 1 x 2 ## Book value ## &lt;int&gt; &lt;chr&gt; ## 1 1 &quot;The Project Gutenberg EBook of The Adventures of Tom Sawyer, Complete\\~ book %&gt;% nchar() ## Book value ## 1 423754 6.2 Find Chapter Splits book = book %&gt;% separate_rows(value, sep = &quot;\\nCHAPTER&quot;) %&gt;% slice(-1) %&gt;% mutate(value = str_remove_all(string = value, pattern = &quot;\\n&quot;)) %&gt;% mutate(value = str_replace(value, &quot;jpg&quot;, &quot;HERE&quot;)) %&gt;% separate(col = &quot;value&quot;, into = c(&quot;Chapter&quot;, &quot;Text&quot;), sep = &quot;HERE&quot;) %&gt;% filter(!is.na(Text)) %&gt;% mutate(Chapter = unlist(str_extract_all(Chapter, &quot;[A-Z]+&quot;))) %&gt;% mutate(Text = str_replace_all(Text, &quot;[.]&quot;,&quot; &quot;)) %&gt;% mutate(Chapter = as.numeric(as.roman(Chapter))) ## Warning: Expected 2 pieces. Additional pieces discarded in 2 rows [55, 60]. ## Warning: Expected 2 pieces. Missing pieces filled with `NA` in 35 rows [1, 2, 3, ## 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, ...]. book ## # A tibble: 35 x 3 ## Book Chapter Text ## &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 1 1 &quot; (182K)\\r\\r\\r\\r\\r\\r“TOM!”\\r\\rNo answer \\r\\r“TOM!”\\r\\rNo answe~ ## 2 1 2 &quot; (202K)\\r\\r\\r\\r\\r\\rSATURDAY morning was come, and all the sum~ ## 3 1 3 &quot; (197K)\\r\\r\\r\\r\\r\\rTOM presented himself before Aunt Polly, w~ ## 4 1 4 &quot; (218K)\\r\\r\\r\\r\\r\\rTHE sun rose upon a tranquil world, and be~ ## 5 1 5 &quot; (205K)\\r\\r\\r\\r\\r\\rABOUT half-past ten the cracked bell of th~ ## 6 1 6 &quot; (202K)\\r\\r\\r\\r\\r\\rMONDAY morning found Tom Sawyer miserable ~ ## 7 1 7 &quot; (175K)\\r\\r\\r\\r\\r\\rTHE harder Tom tried to fasten his mind on~ ## 8 1 8 &quot; (195K)\\r\\r\\r\\r\\r\\rTOM dodged hither and thither through lane~ ## 9 1 9 &quot; (174K)\\r\\r\\r\\r\\r\\rAT half-past nine, that night, Tom and Sid~ ## 10 1 10 &quot; (171K)\\r\\r\\r\\r\\r\\rTHE two boys flew on and on, toward the vi~ ## # ... with 25 more rows 6.3 Tokenize the Book booktokens = book %&gt;% unnest_tokens(word, Text) booktokens ## # A tibble: 70,882 x 3 ## Book Chapter word ## &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 1 1 182k ## 2 1 1 tom ## 3 1 1 no ## 4 1 1 answer ## 5 1 1 tom ## 6 1 1 no ## 7 1 1 answer ## 8 1 1 what’s ## 9 1 1 gone ## 10 1 1 with ## # ... with 70,872 more rows 6.4 Remove ‘stop words’ bookstop = booktokens %&gt;% anti_join(stop_words) ## Joining, by = &quot;word&quot; bookstop ## # A tibble: 26,251 x 3 ## Book Chapter word ## &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 1 1 182k ## 2 1 1 tom ## 3 1 1 answer ## 4 1 1 tom ## 5 1 1 answer ## 6 1 1 what’s ## 7 1 1 boy ## 8 1 1 tom ## 9 1 1 answer ## 10 1 1 lady ## # ... with 26,241 more rows 6.5 Join Sentiments get_sentiments(lexicon = &quot;afinn&quot;) ## # A tibble: 2,477 x 2 ## word value ## &lt;chr&gt; &lt;dbl&gt; ## 1 abandon -2 ## 2 abandoned -2 ## 3 abandons -2 ## 4 abducted -2 ## 5 abduction -2 ## 6 abductions -2 ## 7 abhor -3 ## 8 abhorred -3 ## 9 abhorrent -3 ## 10 abhors -3 ## # ... with 2,467 more rows get_sentiments(lexicon = &quot;bing&quot;) ## # A tibble: 6,786 x 2 ## word sentiment ## &lt;chr&gt; &lt;chr&gt; ## 1 2-faces negative ## 2 abnormal negative ## 3 abolish negative ## 4 abominable negative ## 5 abominably negative ## 6 abominate negative ## 7 abomination negative ## 8 abort negative ## 9 aborted negative ## 10 aborts negative ## # ... with 6,776 more rows get_sentiments(lexicon = &quot;loughran&quot;) ## # A tibble: 4,150 x 2 ## word sentiment ## &lt;chr&gt; &lt;chr&gt; ## 1 abandon negative ## 2 abandoned negative ## 3 abandoning negative ## 4 abandonment negative ## 5 abandonments negative ## 6 abandons negative ## 7 abdicated negative ## 8 abdicates negative ## 9 abdicating negative ## 10 abdication negative ## # ... with 4,140 more rows get_sentiments(lexicon = &quot;nrc&quot;) ## # A tibble: 13,901 x 2 ## word sentiment ## &lt;chr&gt; &lt;chr&gt; ## 1 abacus trust ## 2 abandon fear ## 3 abandon negative ## 4 abandon sadness ## 5 abandoned anger ## 6 abandoned fear ## 7 abandoned negative ## 8 abandoned sadness ## 9 abandonment anger ## 10 abandonment fear ## # ... with 13,891 more rows booktokens %&gt;% left_join(get_sentiments(&quot;bing&quot;)) ## Joining, by = &quot;word&quot; ## # A tibble: 70,882 x 4 ## Book Chapter word sentiment ## &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1 1 182k &lt;NA&gt; ## 2 1 1 tom &lt;NA&gt; ## 3 1 1 no &lt;NA&gt; ## 4 1 1 answer &lt;NA&gt; ## 5 1 1 tom &lt;NA&gt; ## 6 1 1 no &lt;NA&gt; ## 7 1 1 answer &lt;NA&gt; ## 8 1 1 what’s &lt;NA&gt; ## 9 1 1 gone &lt;NA&gt; ## 10 1 1 with &lt;NA&gt; ## # ... with 70,872 more rows booktokens %&gt;% left_join(get_sentiments(&quot;bing&quot;)) %&gt;% filter(!is.na(sentiment)) ## Joining, by = &quot;word&quot; ## # A tibble: 4,778 x 4 ## Book Chapter word sentiment ## &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1 1 wonder positive ## 2 1 1 pride positive ## 3 1 1 well positive ## 4 1 1 perplexed negative ## 5 1 1 loud negative ## 6 1 1 enough positive ## 7 1 1 well positive ## 8 1 1 noise negative ## 9 1 1 slack negative ## 10 1 1 well positive ## # ... with 4,768 more rows 6.6 Descriptive Text Statistics booktokens %&gt;% left_join(get_sentiments(&quot;bing&quot;)) %&gt;% filter(!is.na(sentiment)) %&gt;% count(Chapter,sentiment) ## Joining, by = &quot;word&quot; ## # A tibble: 64 x 3 ## Chapter sentiment n ## &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; ## 1 1 negative 95 ## 2 1 positive 81 ## 3 2 negative 40 ## 4 2 positive 66 ## 5 3 negative 96 ## 6 3 positive 84 ## 7 4 negative 96 ## 8 4 positive 147 ## 9 5 negative 63 ## 10 5 positive 60 ## # ... with 54 more rows 6.7 Visualizations booktokens %&gt;% left_join(get_sentiments(&quot;bing&quot;)) %&gt;% filter(!is.na(sentiment)) %&gt;% count(Chapter,sentiment) %&gt;% mutate(n = if_else(sentiment == &quot;negative&quot;,n*-1,as.double(n))) %&gt;% group_by(Chapter) %&gt;% mutate(order = group_indices()) %&gt;% summarise(n = sum(n)) %&gt;% mutate(pos = if_else(n&gt;0,&quot;pos&quot;,&quot;neg&quot;)) %&gt;% ungroup() %&gt;% ggplot(aes(x=Chapter,y=n,fill = pos, color = pos)) + geom_col() + scale_fill_manual(values = c(&quot;red&quot;,&quot;green&quot;)) + scale_color_manual(values = c(&quot;black&quot;,&quot;black&quot;)) + theme(legend.position=&quot;none&quot;, axis.text.x = element_text(angle = 90)) + labs(y = &quot;Net Positive Words&quot;, title = &quot;Sentiment Analysis of &#39;The Adventures of Tom Sawyer&#39;&quot;, subtitle = &quot;Net Positive Words by Chapter&quot;) ## Joining, by = &quot;word&quot; 6.8 N-Gram Analysis 6.8.1 Uni-Grams booktokens %&gt;% count(word, sort = TRUE) ## # A tibble: 7,774 x 2 ## word n ## &lt;chr&gt; &lt;int&gt; ## 1 the 3708 ## 2 and 3059 ## 3 a 1807 ## 4 to 1696 ## 5 of 1474 ## 6 he 1158 ## 7 was 1126 ## 8 it 1090 ## 9 in 943 ## 10 that 875 ## # ... with 7,764 more rows 6.8.2 Remove Stop Words booktokens %&gt;% left_join(get_sentiments(&quot;bing&quot;)) %&gt;% filter(!is.na(sentiment)) %&gt;% count(word,sentiment, sort = TRUE) ## Joining, by = &quot;word&quot; ## # A tibble: 1,358 x 3 ## word sentiment n ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 well positive 152 ## 2 like positive 113 ## 3 good positive 101 ## 4 work positive 88 ## 5 right positive 83 ## 6 great positive 68 ## 7 dead negative 59 ## 8 enough positive 57 ## 9 poor negative 52 ## 10 cave negative 41 ## # ... with 1,348 more rows 6.8.3 Visualize booktokens %&gt;% left_join(get_sentiments(&quot;bing&quot;)) %&gt;% filter(!is.na(sentiment)) %&gt;% count(word, sentiment, sort = TRUE) %&gt;% group_by(sentiment) %&gt;% top_n(10) %&gt;% ungroup() %&gt;% ggplot(aes(x=fct_reorder(word,n), y = n, fill = sentiment)) + geom_col(show.legend = FALSE) + facet_wrap(~sentiment, scales = &quot;free_y&quot;) + coord_flip() + labs(x=&quot;Word&quot;) ## Joining, by = &quot;word&quot; ## Selecting by n 6.8.4 Bigrams bookbitokens = book %&gt;% unnest_tokens(bigram, Text, token = &quot;ngrams&quot;, n = 2, n_min = 2) bookbitokens ## # A tibble: 70,848 x 3 ## Book Chapter bigram ## &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 1 1 182k tom ## 2 1 1 tom no ## 3 1 1 no answer ## 4 1 1 answer tom ## 5 1 1 tom no ## 6 1 1 no answer ## 7 1 1 answer what’s ## 8 1 1 what’s gone ## 9 1 1 gone with ## 10 1 1 with that ## # ... with 70,838 more rows bookbitokens %&gt;% count(bigram, sort = TRUE) ## # A tibble: 41,080 x 2 ## bigram n ## &lt;chr&gt; &lt;int&gt; ## 1 of the 364 ## 2 in the 298 ## 3 and the 184 ## 4 it was 175 ## 5 to the 175 ## 6 he was 147 ## 7 and then 126 ## 8 was a 116 ## 9 he had 110 ## 10 there was 110 ## # ... with 41,070 more rows 6.8.5 Remove Stop Words in Bigrams bookbitokens %&gt;% separate(bigram, c(&quot;word1&quot;, &quot;word2&quot;), sep = &quot; &quot;) ## # A tibble: 70,848 x 4 ## Book Chapter word1 word2 ## &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1 1 182k tom ## 2 1 1 tom no ## 3 1 1 no answer ## 4 1 1 answer tom ## 5 1 1 tom no ## 6 1 1 no answer ## 7 1 1 answer what’s ## 8 1 1 what’s gone ## 9 1 1 gone with ## 10 1 1 with that ## # ... with 70,838 more rows bigrams = bookbitokens %&gt;% separate(bigram, c(&quot;word1&quot;, &quot;word2&quot;), sep = &quot; &quot;) %&gt;% filter(!word1 %in% stop_words$word) %&gt;% filter(!word2 %in% stop_words$word) bigrams %&gt;% count(word1, word2, sort = TRUE) ## # A tibble: 6,910 x 3 ## word1 word2 n ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 project gutenberg 84 ## 2 gutenberg tm 56 ## 3 injun joe 45 ## 4 aunt polly 42 ## 5 tom sawyer 23 ## 6 injun joe’s 18 ## 7 tm electronic 18 ## 8 muff potter 15 ## 9 archive foundation 13 ## 10 gutenberg literary 13 ## # ... with 6,900 more rows 6.8.6 Visualize Bigrams bigrams %&gt;% unite(col = &quot;bigram&quot;, word1,word2, sep = &quot; &quot;) %&gt;% count(bigram, sort = TRUE) %&gt;% top_n(20) %&gt;% ggplot(aes(x=fct_reorder(bigram,n),y = n)) + geom_col() + coord_flip() + labs(x=&quot;Bigram&quot;,y = &quot;Count&quot;, title = &quot;Top Bigrams&quot;) ## Selecting by n 6.9 Term Frequency Term Frequency: The number of times that a term occurs in the book. Inverse Document Frequency: \\(\\ln(\\frac{Total Number of Documents, cache = TRUE}{Total Number of Documents Containing Specified Word, cache = TRUE})\\): Measure of how much information the word provides. Term Frequency - Inverse Document Frequency: Term Frequency * Inverse Document Frequency 6.9.1 Build TF-IDF Data Words By Chapter booktokens %&gt;% count(Chapter, word, sort = TRUE, name = &quot;count&quot;) %&gt;% add_count(word) %&gt;% spread(Chapter, count) %&gt;% arrange(desc(n)) ## # A tibble: 7,774 x 36 ## word n `1` `2` `3` `4` `5` `6` `7` `8` `9` `10` `11` ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 a 32 66 75 62 132 58 94 34 53 54 52 29 ## 2 all 32 4 8 9 20 7 12 12 6 11 2 3 ## 3 and 32 102 77 110 166 101 139 96 88 99 87 66 ## 4 as 32 16 4 12 18 22 14 10 7 12 10 11 ## 5 be 32 4 5 4 15 7 3 6 11 9 4 5 ## 6 befo~ 32 3 3 4 2 2 3 4 3 2 3 6 ## 7 but 32 19 16 18 22 14 33 22 11 11 18 10 ## 8 for 32 26 11 18 46 27 15 9 6 14 10 12 ## 9 got 32 6 6 6 7 1 10 4 2 5 7 2 ## 10 had 32 13 13 22 27 15 13 8 17 9 8 17 ## # ... with 7,764 more rows, and 23 more variables: `12` &lt;int&gt;, `13` &lt;int&gt;, ## # `14` &lt;int&gt;, `15` &lt;int&gt;, `16` &lt;int&gt;, `17` &lt;int&gt;, `18` &lt;int&gt;, `19` &lt;int&gt;, ## # `20` &lt;int&gt;, `21` &lt;int&gt;, `22` &lt;int&gt;, `23` &lt;int&gt;, `24` &lt;int&gt;, `25` &lt;int&gt;, ## # `27` &lt;int&gt;, `28` &lt;int&gt;, `29` &lt;int&gt;, `30` &lt;int&gt;, `31` &lt;int&gt;, `32` &lt;int&gt;, ## # `33` &lt;int&gt;, `34` &lt;int&gt;, `35` &lt;int&gt; Word Frequency Per Chapter and Book booktokens %&gt;% count(Chapter, word, sort = TRUE, name = &quot;Chapter_Total&quot;) %&gt;% left_join( booktokens %&gt;% count(word, sort = TRUE, name = &quot;Book_Total&quot;) ) ## Joining, by = &quot;word&quot; ## # A tibble: 24,205 x 4 ## Chapter word Chapter_Total Book_Total ## &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; ## 1 16 the 275 3708 ## 2 16 and 269 3059 ## 3 35 the 245 3708 ## 4 33 the 205 3708 ## 5 30 the 187 3708 ## 6 4 the 185 3708 ## 7 5 the 167 3708 ## 8 4 and 166 3059 ## 9 29 the 166 3708 ## 10 21 the 162 3708 ## # ... with 24,195 more rows Create TF-IDF {, cache = TRUE} booktokens %&gt;% count(Chapter, word, sort = TRUE, name = \"Chapter_Total\") %&gt;% left_join( booktokens %&gt;% count(word, sort = TRUE, name = \"Book_Total\") ) %&gt;% bind_tf_idf(word, Chapter, Chapter_Total) %&gt;% filter(Chapter_Total!=Book_Total) %&gt;% filter(tf&lt;1) %&gt;% arrange(-tf_idf) 6.9.2 Visualize TF-IDF booktokens %&gt;% count(Chapter, word, sort = TRUE, name = &quot;Chapter_Total&quot;) %&gt;% left_join( booktokens %&gt;% count(word, sort = TRUE, name = &quot;Book_Total&quot;) ) %&gt;% bind_tf_idf(word, Chapter, Chapter_Total) %&gt;% filter(Chapter_Total!=Book_Total) %&gt;% filter(tf&lt;1) %&gt;% arrange(-tf_idf) %&gt;% group_by(Chapter) %&gt;% top_n(4) %&gt;% ungroup() %&gt;% mutate(word = fct_reorder(word, tf_idf)) %&gt;% ggplot(aes(x = word,y = tf_idf, fill = Chapter)) + geom_col(show.legend = FALSE) + facet_wrap(~Chapter, scales = &quot;free&quot;, ncol = 4) + coord_flip() ## Joining, by = &quot;word&quot; ## Selecting by tf_idf 6.10 Topic Modeling Create Document Term Matrix library(topicmodels) bookdtm = booktokens %&gt;% left_join(get_sentiments(&quot;nrc&quot;)) %&gt;% filter(!is.na(sentiment)) %&gt;% select(Chapter,word) %&gt;% count(Chapter,word) %&gt;% rename(document = Chapter, term = word, count = n) %&gt;% mutate(document = as.integer(document), count = as.double(count)) %&gt;% cast_dtm(document, term, count) ## Joining, by = &quot;word&quot; Create a reproducable example of two topics lda &lt;- LDA(bookdtm, k = 2, control = list(seed = 1234)) lda ## A LDA_VEM topic model with 2 topics. Extract Topics and ‘Beta’ of each topic. Beta represents topic-word density. Beta: In each topic, how dense is this word? Higher is more dense. Lower is less dense topics &lt;- tidy(lda, matrix = &quot;beta&quot;) topics ## # A tibble: 3,526 x 3 ## topic term beta ## &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 advantage 3.10e- 4 ## 2 2 advantage 9.02e-79 ## 3 1 adventurous 1.03e- 4 ## 4 2 adventurous 7.65e-79 ## 5 1 afraid 1.95e- 3 ## 6 2 afraid 2.91e- 3 ## 7 1 arrest 1.03e- 4 ## 8 2 arrest 3.63e-79 ## 9 1 astronomer 4.14e- 4 ## 10 2 astronomer 4.49e-78 ## # ... with 3,516 more rows topics %&gt;% arrange(-beta) ## # A tibble: 3,526 x 3 ## topic term beta ## &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 good 0.0333 ## 2 2 good 0.0279 ## 3 2 found 0.0209 ## 4 2 hope 0.0181 ## 5 1 boy 0.0176 ## 6 2 awful 0.0162 ## 7 1 money 0.0152 ## 8 1 found 0.0119 ## 9 1 aunt 0.0116 ## 10 2 boy 0.0106 ## # ... with 3,516 more rows Top Terms top_terms &lt;- topics %&gt;% group_by(topic) %&gt;% top_n(10, beta) %&gt;% ungroup() %&gt;% arrange(topic, -beta) top_terms ## # A tibble: 20 x 3 ## topic term beta ## &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 good 0.0333 ## 2 1 boy 0.0176 ## 3 1 money 0.0152 ## 4 1 found 0.0119 ## 5 1 aunt 0.0116 ## 6 1 white 0.0105 ## 7 1 time 0.0104 ## 8 1 awful 0.00809 ## 9 1 tree 0.00797 ## 10 1 mother 0.00794 ## 11 2 good 0.0279 ## 12 2 found 0.0209 ## 13 2 hope 0.0181 ## 14 2 awful 0.0162 ## 15 2 boy 0.0106 ## 16 2 time 0.0105 ## 17 2 muff 0.00911 ## 18 2 devil 0.00911 ## 19 2 young 0.00895 ## 20 2 murder 0.00820 top_terms %&gt;% mutate(term = reorder_within(term, beta, topic)) %&gt;% ggplot(aes(term, beta, fill = factor(topic))) + geom_col(show.legend = FALSE) + facet_wrap(~ topic, scales = &quot;free&quot;) + coord_flip() + scale_x_reordered() 6.10.1 Comparison of Use Between Topics beta_spread &lt;- topics %&gt;% mutate(topic = paste0(&quot;topic&quot;, topic)) %&gt;% spread(topic, beta) %&gt;% filter(topic1 &gt; .001 | topic2 &gt; .001) %&gt;% mutate(log_ratio = log2(topic2 / topic1)) beta_spread %&gt;% top_n(10, log_ratio) %&gt;% arrange(-log_ratio) ## # A tibble: 10 x 4 ## term topic1 topic2 log_ratio ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 outburst 4.53e-85 0.00106 270. ## 2 including 9.30e-85 0.00137 270. ## 3 freely 1.25e-84 0.00182 270. ## 4 providing 2.22e-84 0.00243 269. ## 5 fee 2.40e-84 0.00243 269. ## 6 worry 1.29e-84 0.00122 269. ## 7 damages 1.54e-84 0.00122 269. ## 8 agreement 8.21e-84 0.00547 268. ## 9 provide 3.84e-84 0.00213 268. ## 10 information 2.79e-84 0.00122 268. beta_spread %&gt;% top_n(-10, log_ratio) %&gt;% arrange(log_ratio) ## # A tibble: 10 x 4 ## term topic1 topic2 log_ratio ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 labor 0.00259 9.14e-79 -251. ## 2 worship 0.00103 3.53e-78 -247. ## 3 owing 0.00290 1.88e-77 -246. ## 4 cutting 0.00103 7.69e-78 -246. ## 5 highest 0.00124 1.96e-77 -245. ## 6 comrade 0.00166 3.32e-77 -245. ## 7 music 0.00155 6.73e-77 -244. ## 8 grim 0.00124 6.40e-77 -243. ## 9 difficulty 0.00166 1.14e-76 -243. ## 10 indifference 0.00103 1.69e-76 -242. “Gamma”: From the documentation: Each of these values is an estimated proportion of words from that document that are generated from that topic. For example, the model estimates that about 41.7% of the words in document 6 were generated from topic 1. 58.3% of the words in document 6 were generated by topic 2. documents &lt;- tidy(lda, matrix = &quot;gamma&quot;) documents %&gt;% arrange(as.numeric(document)) ## # A tibble: 64 x 3 ## document topic gamma ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1 1 1.00 ## 2 1 2 0.0000677 ## 3 2 1 1.00 ## 4 2 2 0.000114 ## 5 3 1 1.00 ## 6 3 2 0.0000601 ## 7 4 1 1.00 ## 8 4 2 0.0000469 ## 9 5 1 1.00 ## 10 5 2 0.0000702 ## # ... with 54 more rows documents %&gt;% filter(document==6) ## # A tibble: 2 x 3 ## document topic gamma ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 6 1 0.417 ## 2 6 2 0.583 "]
]
