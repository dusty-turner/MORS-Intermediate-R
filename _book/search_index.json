[
["index.html", "Intermediate R 1 Class Introduction 1.1 Expectations 1.2 Class Introductions 1.3 Instructors Introduction 1.4 Course Intent 1.5 Get Course Documents 1.6 Tidy Ecosystem 1.7 Prerequisite Packages", " Intermediate R MAJ Dusty Turner 15 JUN 2020 1 Class Introduction Agenda Data Manipulation Data Visulaization Functional Programming Data Modeling Text Analysis Incorporateing Intermediate R Techniques 1.1 Expectations In taking this course, I’m assuming you have a working knowledge of the following… Base R and possibly ‘tidy’ concepts Computer Coding Statistics Dad Jokes 1.2 Class Introductions Around the room: Where you are from: Who you work for: How you are involved in Data Science: (ie - statistics, coding, application buidling, manager, etc) Why you sad in that seat: Favorite sports team: 1.3 Instructors Introduction 1.3.1 MAJ Dusty Turner Army Combat Engineer Platoon Leader / XO / Company Commander Geospatial / Sapper / Route Clearance Hawaii / White Sands Missile Range / Iraq / Afghanistan Education West Point ’07 Operations Research, BS Missouri University of Science and Technology ’12 Engineering Management, MS THE Ohio State ’16 Integrated Systems Engineering, MS Applied Statistics, Graduate Minor Data Science R User Since ’14 Catch me on Twitter @dtdusty http://dusty-turner.netlify.com/ 1.3.2 Robert Ward 1.4 Course Intent Be interactive Ask questions at any point Don’t let me move too fast (or too slow) Run the code with me 1.5 Get Course Documents [github repo]https://github.com/dusty-turner/MORS-Intermediate-R SSH: \\(git@github.com:dusty-turner/MORS-Intermediate-R.git\\) HTTPS: https://github.com/dusty-turner/MORS-Intermediate-R.git 1.6 Tidy Ecosystem 1.7 Prerequisite Packages "],
["2-data-manipulation.html", "2 Data Manipulation 2.1 Read in Data 2.2 Analysis with dplyr 2.3 Joins 2.4 Other dplyr Tricks", " 2 Data Manipulation library(tidyverse) library(tidymodels) library(janitor) 2.1 Read in Data read_csv(&quot;data_sources/Batting.csv&quot;) ## Parsed with column specification: ## cols( ## .default = col_double(), ## playerID = col_character(), ## teamID = col_character(), ## lgID = col_character(), ## SF = col_logical(), ## GIDP = col_logical() ## ) ## See spec(...) for full column specifications. ## Warning: 45441 parsing failures. ## row col expected actual file ## 25015 GIDP 1/0/T/F/TRUE/FALSE 2 &#39;data_sources/Batting.csv&#39; ## 25016 GIDP 1/0/T/F/TRUE/FALSE 10 &#39;data_sources/Batting.csv&#39; ## 25018 GIDP 1/0/T/F/TRUE/FALSE 4 &#39;data_sources/Batting.csv&#39; ## 25028 GIDP 1/0/T/F/TRUE/FALSE 8 &#39;data_sources/Batting.csv&#39; ## 25030 GIDP 1/0/T/F/TRUE/FALSE 3 &#39;data_sources/Batting.csv&#39; ## ..... .... .................. ...... .......................... ## See problems(...) for more details. ## # A tibble: 102,816 x 22 ## playerID yearID stint teamID lgID G AB R H `2B` `3B` HR ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 abercda~ 1871 1 TRO &lt;NA&gt; 1 4 0 0 0 0 0 ## 2 addybo01 1871 1 RC1 &lt;NA&gt; 25 118 30 32 6 0 0 ## 3 allisar~ 1871 1 CL1 &lt;NA&gt; 29 137 28 40 4 5 0 ## 4 allisdo~ 1871 1 WS3 &lt;NA&gt; 27 133 28 44 10 2 2 ## 5 ansonca~ 1871 1 RC1 &lt;NA&gt; 25 120 29 39 11 3 0 ## 6 armstbo~ 1871 1 FW1 &lt;NA&gt; 12 49 9 11 2 1 0 ## 7 barkeal~ 1871 1 RC1 &lt;NA&gt; 1 4 0 1 0 0 0 ## 8 barnero~ 1871 1 BS1 &lt;NA&gt; 31 157 66 63 10 9 0 ## 9 barrebi~ 1871 1 FW1 &lt;NA&gt; 1 5 1 1 1 0 0 ## 10 barrofr~ 1871 1 BS1 &lt;NA&gt; 18 86 13 13 2 1 0 ## # ... with 102,806 more rows, and 10 more variables: RBI &lt;dbl&gt;, SB &lt;dbl&gt;, ## # CS &lt;dbl&gt;, BB &lt;dbl&gt;, SO &lt;dbl&gt;, IBB &lt;dbl&gt;, HBP &lt;dbl&gt;, SH &lt;dbl&gt;, SF &lt;lgl&gt;, ## # GIDP &lt;lgl&gt; Fix Read In Errors read_csv(&quot;data_sources/Batting.csv&quot;, col_types = cols(SF = col_double(), GIDP = col_double())) ## # A tibble: 102,816 x 22 ## playerID yearID stint teamID lgID G AB R H `2B` `3B` HR ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 abercda~ 1871 1 TRO &lt;NA&gt; 1 4 0 0 0 0 0 ## 2 addybo01 1871 1 RC1 &lt;NA&gt; 25 118 30 32 6 0 0 ## 3 allisar~ 1871 1 CL1 &lt;NA&gt; 29 137 28 40 4 5 0 ## 4 allisdo~ 1871 1 WS3 &lt;NA&gt; 27 133 28 44 10 2 2 ## 5 ansonca~ 1871 1 RC1 &lt;NA&gt; 25 120 29 39 11 3 0 ## 6 armstbo~ 1871 1 FW1 &lt;NA&gt; 12 49 9 11 2 1 0 ## 7 barkeal~ 1871 1 RC1 &lt;NA&gt; 1 4 0 1 0 0 0 ## 8 barnero~ 1871 1 BS1 &lt;NA&gt; 31 157 66 63 10 9 0 ## 9 barrebi~ 1871 1 FW1 &lt;NA&gt; 1 5 1 1 1 0 0 ## 10 barrofr~ 1871 1 BS1 &lt;NA&gt; 18 86 13 13 2 1 0 ## # ... with 102,806 more rows, and 10 more variables: RBI &lt;dbl&gt;, SB &lt;dbl&gt;, ## # CS &lt;dbl&gt;, BB &lt;dbl&gt;, SO &lt;dbl&gt;, IBB &lt;dbl&gt;, HBP &lt;dbl&gt;, SH &lt;dbl&gt;, SF &lt;dbl&gt;, ## # GIDP &lt;dbl&gt; read_csv(&quot;data_sources/Batting.csv&quot;, guess_max = 10000) ## Parsed with column specification: ## cols( ## .default = col_double(), ## playerID = col_character(), ## teamID = col_character(), ## lgID = col_character() ## ) ## See spec(...) for full column specifications. ## # A tibble: 102,816 x 22 ## playerID yearID stint teamID lgID G AB R H `2B` `3B` HR ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 abercda~ 1871 1 TRO &lt;NA&gt; 1 4 0 0 0 0 0 ## 2 addybo01 1871 1 RC1 &lt;NA&gt; 25 118 30 32 6 0 0 ## 3 allisar~ 1871 1 CL1 &lt;NA&gt; 29 137 28 40 4 5 0 ## 4 allisdo~ 1871 1 WS3 &lt;NA&gt; 27 133 28 44 10 2 2 ## 5 ansonca~ 1871 1 RC1 &lt;NA&gt; 25 120 29 39 11 3 0 ## 6 armstbo~ 1871 1 FW1 &lt;NA&gt; 12 49 9 11 2 1 0 ## 7 barkeal~ 1871 1 RC1 &lt;NA&gt; 1 4 0 1 0 0 0 ## 8 barnero~ 1871 1 BS1 &lt;NA&gt; 31 157 66 63 10 9 0 ## 9 barrebi~ 1871 1 FW1 &lt;NA&gt; 1 5 1 1 1 0 0 ## 10 barrofr~ 1871 1 BS1 &lt;NA&gt; 18 86 13 13 2 1 0 ## # ... with 102,806 more rows, and 10 more variables: RBI &lt;dbl&gt;, SB &lt;dbl&gt;, ## # CS &lt;dbl&gt;, BB &lt;dbl&gt;, SO &lt;dbl&gt;, IBB &lt;dbl&gt;, HBP &lt;dbl&gt;, SH &lt;dbl&gt;, SF &lt;dbl&gt;, ## # GIDP &lt;dbl&gt; Clean Column Names read_csv(&quot;data_sources/Batting.csv&quot;, col_types = cols(SF = col_double(), GIDP = col_double())) %&gt;% clean_names() ## # A tibble: 102,816 x 22 ## player_id year_id stint team_id lg_id g ab r h x2b x3b ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 abercda01 1871 1 TRO &lt;NA&gt; 1 4 0 0 0 0 ## 2 addybo01 1871 1 RC1 &lt;NA&gt; 25 118 30 32 6 0 ## 3 allisar01 1871 1 CL1 &lt;NA&gt; 29 137 28 40 4 5 ## 4 allisdo01 1871 1 WS3 &lt;NA&gt; 27 133 28 44 10 2 ## 5 ansonca01 1871 1 RC1 &lt;NA&gt; 25 120 29 39 11 3 ## 6 armstbo01 1871 1 FW1 &lt;NA&gt; 12 49 9 11 2 1 ## 7 barkeal01 1871 1 RC1 &lt;NA&gt; 1 4 0 1 0 0 ## 8 barnero01 1871 1 BS1 &lt;NA&gt; 31 157 66 63 10 9 ## 9 barrebi01 1871 1 FW1 &lt;NA&gt; 1 5 1 1 1 0 ## 10 barrofr01 1871 1 BS1 &lt;NA&gt; 18 86 13 13 2 1 ## # ... with 102,806 more rows, and 11 more variables: hr &lt;dbl&gt;, rbi &lt;dbl&gt;, ## # sb &lt;dbl&gt;, cs &lt;dbl&gt;, bb &lt;dbl&gt;, so &lt;dbl&gt;, ibb &lt;dbl&gt;, hbp &lt;dbl&gt;, sh &lt;dbl&gt;, ## # sf &lt;dbl&gt;, gidp &lt;dbl&gt; 2.2 Analysis with dplyr Who has the highest career slugging percentage? Select data = read_csv(&quot;data_sources/Batting.csv&quot;, col_types = cols(SF = col_double(), GIDP = col_double())) %&gt;% clean_names() data %&gt;% select(player_id,year_id,h,x2b,x3b,hr,ab, g) ## # A tibble: 102,816 x 8 ## player_id year_id h x2b x3b hr ab g ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 abercda01 1871 0 0 0 0 4 1 ## 2 addybo01 1871 32 6 0 0 118 25 ## 3 allisar01 1871 40 4 5 0 137 29 ## 4 allisdo01 1871 44 10 2 2 133 27 ## 5 ansonca01 1871 39 11 3 0 120 25 ## 6 armstbo01 1871 11 2 1 0 49 12 ## 7 barkeal01 1871 1 0 0 0 4 1 ## 8 barnero01 1871 63 10 9 0 157 31 ## 9 barrebi01 1871 1 1 0 0 5 1 ## 10 barrofr01 1871 13 2 1 0 86 18 ## # ... with 102,806 more rows Group_by data %&gt;% select(player_id,year_id,h,x2b,x3b,hr,ab, g) %&gt;% group_by(player_id) ## # A tibble: 102,816 x 8 ## # Groups: player_id [18,915] ## player_id year_id h x2b x3b hr ab g ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 abercda01 1871 0 0 0 0 4 1 ## 2 addybo01 1871 32 6 0 0 118 25 ## 3 allisar01 1871 40 4 5 0 137 29 ## 4 allisdo01 1871 44 10 2 2 133 27 ## 5 ansonca01 1871 39 11 3 0 120 25 ## 6 armstbo01 1871 11 2 1 0 49 12 ## 7 barkeal01 1871 1 0 0 0 4 1 ## 8 barnero01 1871 63 10 9 0 157 31 ## 9 barrebi01 1871 1 1 0 0 5 1 ## 10 barrofr01 1871 13 2 1 0 86 18 ## # ... with 102,806 more rows Summarise data %&gt;% select(player_id,year_id,h,x2b,x3b,hr,ab, g) %&gt;% group_by(player_id) %&gt;% summarise(h = sum(h), x2b = sum(x2b), x3b = sum(x3b), hr = sum(hr), ab = sum(ab), g = sum(g)) ## `summarise()` ungrouping output (override with `.groups` argument) ## # A tibble: 18,915 x 7 ## player_id h x2b x3b hr ab g ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 aardsda01 0 0 0 0 4 331 ## 2 aaronha01 3771 624 98 755 12364 3298 ## 3 aaronto01 216 42 6 13 944 437 ## 4 aasedo01 0 0 0 0 5 448 ## 5 abadan01 2 0 0 0 21 15 ## 6 abadfe01 1 0 0 0 9 315 ## 7 abadijo01 11 0 0 0 49 12 ## 8 abbated01 772 99 43 11 3044 855 ## 9 abbeybe01 38 3 3 0 225 79 ## 10 abbeych01 492 67 46 19 1751 451 ## # ... with 18,905 more rows Filter data %&gt;% select(player_id,year_id,h,x2b,x3b,hr,ab, g) %&gt;% group_by(player_id) %&gt;% summarise(h = sum(h), x2b = sum(x2b), x3b = sum(x3b), hr = sum(hr), ab = sum(ab), g = sum(g)) %&gt;% filter(g&gt;1000) ## # A tibble: 1,564 x 7 ## player_id h x2b x3b hr ab g ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 aaronha01 3771 624 98 755 12364 3298 ## 2 abreubo01 2470 574 59 288 8480 2425 ## 3 adairje01 1022 163 19 57 4019 1165 ## 4 adamsbo03 1082 188 49 37 4019 1281 ## 5 adamssp01 1588 249 48 9 5557 1424 ## 6 adcocjo01 1832 295 35 336 6606 1959 ## 7 ageeto01 999 170 27 130 3912 1129 ## 8 ainsmed01 707 108 54 22 3048 1078 ## 9 alfoned01 1532 282 18 146 5385 1506 ## 10 alicelu01 1031 189 53 47 3971 1341 ## # ... with 1,554 more rows Mutate data %&gt;% select(player_id,year_id,h,x2b,x3b,hr,ab, g) %&gt;% group_by(player_id) %&gt;% summarise(h = sum(h), x2b = sum(x2b), x3b = sum(x3b), hr = sum(hr), ab = sum(ab), g = sum(g)) %&gt;% filter(g&gt;1000) %&gt;% mutate(slg = (h + x2b + 2*x3b + 3*hr)/ab) ## # A tibble: 1,564 x 8 ## player_id h x2b x3b hr ab g slg ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 aaronha01 3771 624 98 755 12364 3298 0.555 ## 2 abreubo01 2470 574 59 288 8480 2425 0.475 ## 3 adairje01 1022 163 19 57 4019 1165 0.347 ## 4 adamsbo03 1082 188 49 37 4019 1281 0.368 ## 5 adamssp01 1588 249 48 9 5557 1424 0.353 ## 6 adcocjo01 1832 295 35 336 6606 1959 0.485 ## 7 ageeto01 999 170 27 130 3912 1129 0.412 ## 8 ainsmed01 707 108 54 22 3048 1078 0.324 ## 9 alfoned01 1532 282 18 146 5385 1506 0.425 ## 10 alicelu01 1031 189 53 47 3971 1341 0.369 ## # ... with 1,554 more rows Arrange slgdata = data %&gt;% select(player_id,year_id,h,x2b,x3b,hr,ab, g) %&gt;% group_by(player_id) %&gt;% summarise(h = sum(h), x2b = sum(x2b), x3b = sum(x3b), hr = sum(hr), ab = sum(ab), g = sum(g)) %&gt;% filter(g&gt;1000) %&gt;% mutate(slg = (h + x2b + 2*x3b + 3*hr)/ab) %&gt;% arrange(desc(slg)) slgdata ## # A tibble: 1,564 x 8 ## player_id h x2b x3b hr ab g slg ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 ruthba01 2873 506 136 714 8398 2503 0.690 ## 2 willite01 2654 525 71 521 7706 2292 0.634 ## 3 gehrilo01 2721 534 163 493 8001 2164 0.632 ## 4 foxxji01 2646 458 125 534 8134 2317 0.609 ## 5 bondsba01 2935 601 77 762 9847 2986 0.607 ## 6 greenha01 1628 379 71 331 5193 1394 0.605 ## 7 mcgwima01 1626 252 6 583 6187 1874 0.588 ## 8 ramirma02 2574 547 20 555 8244 2302 0.585 ## 9 dimagjo01 2214 389 131 361 6821 1736 0.579 ## 10 hornsro01 2930 541 169 301 8173 2259 0.577 ## # ... with 1,554 more rows 2.3 Joins Player ID Data read_csv(&quot;data_sources/Master.csv&quot;) ## Parsed with column specification: ## cols( ## .default = col_character(), ## birthYear = col_double(), ## birthMonth = col_double(), ## birthDay = col_double(), ## deathYear = col_double(), ## deathMonth = col_double(), ## deathDay = col_double(), ## weight = col_double(), ## height = col_double(), ## debut = col_date(format = &quot;&quot;), ## finalGame = col_date(format = &quot;&quot;) ## ) ## See spec(...) for full column specifications. ## # A tibble: 19,105 x 24 ## playerID birthYear birthMonth birthDay birthCountry birthState birthCity ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 aardsda~ 1981 12 27 USA CO Denver ## 2 aaronha~ 1934 2 5 USA AL Mobile ## 3 aaronto~ 1939 8 5 USA AL Mobile ## 4 aasedo01 1954 9 8 USA CA Orange ## 5 abadan01 1972 8 25 USA FL Palm Bea~ ## 6 abadfe01 1985 12 17 D.R. La Romana La Romana ## 7 abadijo~ 1850 11 4 USA PA Philadel~ ## 8 abbated~ 1877 4 15 USA PA Latrobe ## 9 abbeybe~ 1869 11 11 USA VT Essex ## 10 abbeych~ 1866 10 14 USA NE Falls Ci~ ## # ... with 19,095 more rows, and 17 more variables: deathYear &lt;dbl&gt;, ## # deathMonth &lt;dbl&gt;, deathDay &lt;dbl&gt;, deathCountry &lt;chr&gt;, deathState &lt;chr&gt;, ## # deathCity &lt;chr&gt;, nameFirst &lt;chr&gt;, nameLast &lt;chr&gt;, nameGiven &lt;chr&gt;, ## # weight &lt;dbl&gt;, height &lt;dbl&gt;, bats &lt;chr&gt;, throws &lt;chr&gt;, debut &lt;date&gt;, ## # finalGame &lt;date&gt;, retroID &lt;chr&gt;, bbrefID &lt;chr&gt; read_csv(&quot;data_sources/Master.csv&quot;) %&gt;% select(playerID, nameFirst, nameLast) %&gt;% mutate(player = paste(nameFirst,nameLast)) %&gt;% select(-starts_with(&quot;name&quot;)) ## Parsed with column specification: ## cols( ## .default = col_character(), ## birthYear = col_double(), ## birthMonth = col_double(), ## birthDay = col_double(), ## deathYear = col_double(), ## deathMonth = col_double(), ## deathDay = col_double(), ## weight = col_double(), ## height = col_double(), ## debut = col_date(format = &quot;&quot;), ## finalGame = col_date(format = &quot;&quot;) ## ) ## See spec(...) for full column specifications. ## # A tibble: 19,105 x 2 ## playerID player ## &lt;chr&gt; &lt;chr&gt; ## 1 aardsda01 David Aardsma ## 2 aaronha01 Hank Aaron ## 3 aaronto01 Tommie Aaron ## 4 aasedo01 Don Aase ## 5 abadan01 Andy Abad ## 6 abadfe01 Fernando Abad ## 7 abadijo01 John Abadie ## 8 abbated01 Ed Abbaticchio ## 9 abbeybe01 Bert Abbey ## 10 abbeych01 Charlie Abbey ## # ... with 19,095 more rows master = read_csv(&quot;data_sources/Master.csv&quot;) %&gt;% select(playerID, nameFirst, nameLast) %&gt;% mutate(player = paste(nameFirst,nameLast)) %&gt;% select(-starts_with(&quot;name&quot;)) ## Parsed with column specification: ## cols( ## .default = col_character(), ## birthYear = col_double(), ## birthMonth = col_double(), ## birthDay = col_double(), ## deathYear = col_double(), ## deathMonth = col_double(), ## deathDay = col_double(), ## weight = col_double(), ## height = col_double(), ## debut = col_date(format = &quot;&quot;), ## finalGame = col_date(format = &quot;&quot;) ## ) ## See spec(...) for full column specifications. Join the Data slgdata %&gt;% left_join(master, by = c(&quot;player_id&quot; = &quot;playerID&quot;)) ## # A tibble: 1,564 x 9 ## player_id h x2b x3b hr ab g slg player ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 ruthba01 2873 506 136 714 8398 2503 0.690 Babe Ruth ## 2 willite01 2654 525 71 521 7706 2292 0.634 Ted Williams ## 3 gehrilo01 2721 534 163 493 8001 2164 0.632 Lou Gehrig ## 4 foxxji01 2646 458 125 534 8134 2317 0.609 Jimmie Foxx ## 5 bondsba01 2935 601 77 762 9847 2986 0.607 Barry Bonds ## 6 greenha01 1628 379 71 331 5193 1394 0.605 Hank Greenberg ## 7 mcgwima01 1626 252 6 583 6187 1874 0.588 Mark McGwire ## 8 ramirma02 2574 547 20 555 8244 2302 0.585 Manny Ramirez ## 9 dimagjo01 2214 389 131 361 6821 1736 0.579 Joe DiMaggio ## 10 hornsro01 2930 541 169 301 8173 2259 0.577 Rogers Hornsby ## # ... with 1,554 more rows slgname = slgdata %&gt;% left_join(master, by = c(&quot;player_id&quot; = &quot;playerID&quot;)) Rename Columns slgname %&gt;% rename(doubles = x2b, triples = x3b) ## # A tibble: 1,564 x 9 ## player_id h doubles triples hr ab g slg player ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 ruthba01 2873 506 136 714 8398 2503 0.690 Babe Ruth ## 2 willite01 2654 525 71 521 7706 2292 0.634 Ted Williams ## 3 gehrilo01 2721 534 163 493 8001 2164 0.632 Lou Gehrig ## 4 foxxji01 2646 458 125 534 8134 2317 0.609 Jimmie Foxx ## 5 bondsba01 2935 601 77 762 9847 2986 0.607 Barry Bonds ## 6 greenha01 1628 379 71 331 5193 1394 0.605 Hank Greenberg ## 7 mcgwima01 1626 252 6 583 6187 1874 0.588 Mark McGwire ## 8 ramirma02 2574 547 20 555 8244 2302 0.585 Manny Ramirez ## 9 dimagjo01 2214 389 131 361 6821 1736 0.579 Joe DiMaggio ## 10 hornsro01 2930 541 169 301 8173 2259 0.577 Rogers Hornsby ## # ... with 1,554 more rows slgname = slgname %&gt;% rename(doubles = x2b, triples = x3b) Reorder Columns slgname %&gt;% select(player_id, player, everything()) ## # A tibble: 1,564 x 9 ## player_id player h doubles triples hr ab g slg ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 ruthba01 Babe Ruth 2873 506 136 714 8398 2503 0.690 ## 2 willite01 Ted Williams 2654 525 71 521 7706 2292 0.634 ## 3 gehrilo01 Lou Gehrig 2721 534 163 493 8001 2164 0.632 ## 4 foxxji01 Jimmie Foxx 2646 458 125 534 8134 2317 0.609 ## 5 bondsba01 Barry Bonds 2935 601 77 762 9847 2986 0.607 ## 6 greenha01 Hank Greenberg 1628 379 71 331 5193 1394 0.605 ## 7 mcgwima01 Mark McGwire 1626 252 6 583 6187 1874 0.588 ## 8 ramirma02 Manny Ramirez 2574 547 20 555 8244 2302 0.585 ## 9 dimagjo01 Joe DiMaggio 2214 389 131 361 6821 1736 0.579 ## 10 hornsro01 Rogers Hornsby 2930 541 169 301 8173 2259 0.577 ## # ... with 1,554 more rows slgname = slgname %&gt;% select(player_id, player, everything()) 2.4 Other dplyr Tricks Count data ## # A tibble: 102,816 x 22 ## player_id year_id stint team_id lg_id g ab r h x2b x3b ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 abercda01 1871 1 TRO &lt;NA&gt; 1 4 0 0 0 0 ## 2 addybo01 1871 1 RC1 &lt;NA&gt; 25 118 30 32 6 0 ## 3 allisar01 1871 1 CL1 &lt;NA&gt; 29 137 28 40 4 5 ## 4 allisdo01 1871 1 WS3 &lt;NA&gt; 27 133 28 44 10 2 ## 5 ansonca01 1871 1 RC1 &lt;NA&gt; 25 120 29 39 11 3 ## 6 armstbo01 1871 1 FW1 &lt;NA&gt; 12 49 9 11 2 1 ## 7 barkeal01 1871 1 RC1 &lt;NA&gt; 1 4 0 1 0 0 ## 8 barnero01 1871 1 BS1 &lt;NA&gt; 31 157 66 63 10 9 ## 9 barrebi01 1871 1 FW1 &lt;NA&gt; 1 5 1 1 1 0 ## 10 barrofr01 1871 1 BS1 &lt;NA&gt; 18 86 13 13 2 1 ## # ... with 102,806 more rows, and 11 more variables: hr &lt;dbl&gt;, rbi &lt;dbl&gt;, ## # sb &lt;dbl&gt;, cs &lt;dbl&gt;, bb &lt;dbl&gt;, so &lt;dbl&gt;, ibb &lt;dbl&gt;, hbp &lt;dbl&gt;, sh &lt;dbl&gt;, ## # sf &lt;dbl&gt;, gidp &lt;dbl&gt; data %&gt;% count(player_id) ## # A tibble: 18,915 x 2 ## player_id n ## &lt;chr&gt; &lt;int&gt; ## 1 aardsda01 9 ## 2 aaronha01 23 ## 3 aaronto01 7 ## 4 aasedo01 13 ## 5 abadan01 3 ## 6 abadfe01 8 ## 7 abadijo01 2 ## 8 abbated01 10 ## 9 abbeybe01 6 ## 10 abbeych01 5 ## # ... with 18,905 more rows data %&gt;% count(player_id, sort = TRUE) ## # A tibble: 18,915 x 2 ## player_id n ## &lt;chr&gt; &lt;int&gt; ## 1 mcguide01 31 ## 2 henderi01 29 ## 3 newsobo01 29 ## 4 johnto01 28 ## 5 kaatji01 28 ## 6 ansonca01 27 ## 7 baineha01 27 ## 8 carltst01 27 ## 9 moyerja01 27 ## 10 ryanno01 27 ## # ... with 18,905 more rows Uncount slgname %&gt;% uncount(triples) ## # A tibble: 80,303 x 8 ## player_id player h doubles hr ab g slg ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 ruthba01 Babe Ruth 2873 506 714 8398 2503 0.690 ## 2 ruthba01 Babe Ruth 2873 506 714 8398 2503 0.690 ## 3 ruthba01 Babe Ruth 2873 506 714 8398 2503 0.690 ## 4 ruthba01 Babe Ruth 2873 506 714 8398 2503 0.690 ## 5 ruthba01 Babe Ruth 2873 506 714 8398 2503 0.690 ## 6 ruthba01 Babe Ruth 2873 506 714 8398 2503 0.690 ## 7 ruthba01 Babe Ruth 2873 506 714 8398 2503 0.690 ## 8 ruthba01 Babe Ruth 2873 506 714 8398 2503 0.690 ## 9 ruthba01 Babe Ruth 2873 506 714 8398 2503 0.690 ## 10 ruthba01 Babe Ruth 2873 506 714 8398 2503 0.690 ## # ... with 80,293 more rows Summarise data %&gt;% select(player_id,year_id,h,x2b,x3b,hr,ab, g) %&gt;% group_by(player_id) %&gt;% summarise(h = sum(h), x2b = sum(x2b), x3b = sum(x3b), hr = sum(hr), ab = sum(ab), g = sum(g)) ## `summarise()` ungrouping output (override with `.groups` argument) ## # A tibble: 18,915 x 7 ## player_id h x2b x3b hr ab g ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 aardsda01 0 0 0 0 4 331 ## 2 aaronha01 3771 624 98 755 12364 3298 ## 3 aaronto01 216 42 6 13 944 437 ## 4 aasedo01 0 0 0 0 5 448 ## 5 abadan01 2 0 0 0 21 15 ## 6 abadfe01 1 0 0 0 9 315 ## 7 abadijo01 11 0 0 0 49 12 ## 8 abbated01 772 99 43 11 3044 855 ## 9 abbeybe01 38 3 3 0 225 79 ## 10 abbeych01 492 67 46 19 1751 451 ## # ... with 18,905 more rows data %&gt;% select(player_id,year_id,h,x2b,x3b,hr,ab, g) %&gt;% group_by(player_id) %&gt;% summarise_at(vars(-year_id),~sum(.)) ## # A tibble: 18,915 x 7 ## player_id h x2b x3b hr ab g ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 aardsda01 0 0 0 0 4 331 ## 2 aaronha01 3771 624 98 755 12364 3298 ## 3 aaronto01 216 42 6 13 944 437 ## 4 aasedo01 0 0 0 0 5 448 ## 5 abadan01 2 0 0 0 21 15 ## 6 abadfe01 1 0 0 0 9 315 ## 7 abadijo01 11 0 0 0 49 12 ## 8 abbated01 772 99 43 11 3044 855 ## 9 abbeybe01 38 3 3 0 225 79 ## 10 abbeych01 492 67 46 19 1751 451 ## # ... with 18,905 more rows "],
["3-data-visualization.html", "3 Data Visualization 3.1 Bar Chart Scatter Plot 3.2 Animation", " 3 Data Visualization 3.1 Bar Chart slgname ## # A tibble: 1,564 x 9 ## player_id player h doubles triples hr ab g slg ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 ruthba01 Babe Ruth 2873 506 136 714 8398 2503 0.690 ## 2 willite01 Ted Williams 2654 525 71 521 7706 2292 0.634 ## 3 gehrilo01 Lou Gehrig 2721 534 163 493 8001 2164 0.632 ## 4 foxxji01 Jimmie Foxx 2646 458 125 534 8134 2317 0.609 ## 5 bondsba01 Barry Bonds 2935 601 77 762 9847 2986 0.607 ## 6 greenha01 Hank Greenberg 1628 379 71 331 5193 1394 0.605 ## 7 mcgwima01 Mark McGwire 1626 252 6 583 6187 1874 0.588 ## 8 ramirma02 Manny Ramirez 2574 547 20 555 8244 2302 0.585 ## 9 dimagjo01 Joe DiMaggio 2214 389 131 361 6821 1736 0.579 ## 10 hornsro01 Rogers Hornsby 2930 541 169 301 8173 2259 0.577 ## # ... with 1,554 more rows Canvas slgname %&gt;% ggplot() Mapping slgname %&gt;% ggplot(aes(x=player, y = hr)) Filter Data slgname %&gt;% top_n(20, h) %&gt;% ggplot(aes(x=player, y = hr)) Plot ‘mechanism’ slgname %&gt;% top_n(20, h) %&gt;% ggplot(aes(x=player, y = hr)) + geom_col() Coordinate Flip slgname %&gt;% top_n(20, h) %&gt;% ggplot(aes(x=player, y = hr)) + geom_col() + coord_flip() Reorder Factors slgname %&gt;% top_n(20, h) %&gt;% ggplot(aes(x=fct_reorder(player,hr), y = hr)) + geom_col() + coord_flip() Color Bars slgname %&gt;% top_n(20, h) %&gt;% ggplot(aes(x=fct_reorder(player,hr), y = hr, fill = slg)) + geom_col() + coord_flip() Update Labels slgname %&gt;% top_n(20, h) %&gt;% ggplot(aes(x=fct_reorder(player,hr), y = hr, fill = slg)) + geom_col() + coord_flip() + labs(x = &quot;Player&quot;, y = &quot;Home Runs&quot;, title = &quot;Top 20 Home Run Hitters&quot;, fill = &quot;Slugging Percentage&quot;, subtitle = &quot;1871-2016&quot;, caption = &quot;*Among Players Who&#39;ve Played at least 1000 games&quot;) Scatter Plot scatterdat = data %&gt;% filter(lg_id == &quot;AL&quot;) %&gt;% mutate(team_id = fct_lump(team_id, n = 12)) %&gt;% filter(team_id != &quot;Other&quot;) %&gt;% filter(complete.cases(.)) %&gt;% filter(year_id&gt;1990) %&gt;% group_by(team_id,year_id) %&gt;% summarise_at(vars(g:gidp), ~sum(.)) scatterdat ## # A tibble: 260 x 19 ## # Groups: team_id [10] ## team_id year_id g ab r h x2b x3b hr rbi sb cs ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 BAL 1991 2365 5604 686 1421 256 29 170 660 50 33 ## 2 BAL 1992 2183 5485 705 1423 243 36 148 680 89 48 ## 3 BAL 1993 2127 5508 786 1470 287 24 157 744 73 54 ## 4 BAL 1994 1473 3856 589 1047 185 20 139 557 69 13 ## 5 BAL 1995 2046 4837 704 1267 229 27 173 668 92 45 ## 6 BAL 1996 2245 5689 949 1557 299 29 257 914 76 40 ## 7 BAL 1997 2282 5584 812 1498 264 22 196 780 63 26 ## 8 BAL 1998 2359 5565 817 1520 303 11 214 783 86 48 ## 9 BAL 1999 2300 5637 851 1572 299 21 203 804 107 46 ## 10 BAL 2000 2210 5549 794 1508 310 22 184 750 126 65 ## # ... with 250 more rows, and 7 more variables: bb &lt;dbl&gt;, so &lt;dbl&gt;, ibb &lt;dbl&gt;, ## # hbp &lt;dbl&gt;, sh &lt;dbl&gt;, sf &lt;dbl&gt;, gidp &lt;dbl&gt; Canvas scatterdat %&gt;% ggplot() Mapping scatterdat %&gt;% ggplot(aes(x=year_id,y=hr,color = fct_reorder(team_id,-hr))) Plot ‘mechanism’: geom_point() scatterdat %&gt;% ggplot(aes(x=year_id,y=hr,color = fct_reorder(team_id,-hr))) + geom_point() Plot ‘mechanism’: geom_smooth() scatterdat %&gt;% ggplot(aes(x=year_id,y=hr)) + geom_point(aes(color = fct_reorder(team_id,-hr))) + geom_smooth() ## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; Facet Plots scatterdat %&gt;% ggplot(aes(x=year_id,y=hr)) + geom_point(aes(color = fct_reorder(team_id,-hr))) + geom_smooth() + facet_wrap(~team_id) ## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; Update Labels scatterdat %&gt;% ggplot(aes(x=year_id,y=hr)) + geom_point(aes(color = fct_reorder(team_id,-hr))) + geom_smooth() + facet_wrap(~team_id) + labs(title = &quot;Home Run Changes Over Time&quot;, subtitle = paste(&quot;From&quot;, min(scatterdat$year_id), &quot;to&quot;, max(scatterdat$year_id)), color = &quot;Team&quot;, x = &quot;Season&quot;, y = &quot;Season Home Run Total&quot;) ## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; Change Smoothing Line scatterdat %&gt;% ggplot(aes(x=year_id,y=hr)) + geom_point(aes(color = fct_reorder(team_id,-hr))) + geom_smooth(method = &quot;lm&quot;) + facet_wrap(~team_id) + labs(title = &quot;Home Run Changes Over Time&quot;, subtitle = paste(&quot;From&quot;, min(scatterdat$year_id), &quot;to&quot;, max(scatterdat$year_id)), color = &quot;Team&quot;, x = &quot;Season&quot;, y = &quot;Season Home Run Total&quot;) ## `geom_smooth()` using formula &#39;y ~ x&#39; 3.2 Animation library(gganimate) animdata = data %&gt;% filter(!is.na(lg_id)) %&gt;% group_by(year_id, team_id) %&gt;% summarise_at(vars(hr, so, bb, ab), ~ sum(., na.rm = TRUE)) %&gt;% ungroup() %&gt;% inner_join(data %&gt;% select(year_id, team_id, lg_id) %&gt;% distinct()) animdata ## # A tibble: 2,785 x 7 ## year_id team_id hr so bb ab lg_id ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 1876 BSN 9 98 58 2722 NL ## 2 1876 CHN 8 45 70 2748 NL ## 3 1876 CN1 4 136 41 2372 NL ## 4 1876 HAR 2 78 39 2664 NL ## 5 1876 LS1 6 98 24 2570 NL ## 6 1876 NY3 2 35 18 2180 NL ## 7 1876 PHN 7 36 27 2387 NL ## 8 1876 SL3 2 63 59 2478 NL ## 9 1877 BSN 4 121 65 2368 NL ## 10 1877 CHN 0 111 57 2273 NL ## # ... with 2,775 more rows p = animdata %&gt;% ggplot(aes(x = so,y = hr, color = bb,size = ab,group = year_id)) + geom_point() + facet_wrap(~ lg_id) + transition_states(year_id,transition_length = 1,state_length = 30) + labs(title = &#39;Year: {closest_state, cache = TRUE}&#39;) + enter_fade() + exit_fade() animate(p,nframes = length(unique(data$year_id)) * 2) "],
["4-functional-programming.html", "4 Functional Programming 4.1 Iteration 4.2 An Interesting Question 4.3 A More General Question 4.4 Even more Genarally 4.5 Even more Genarally 4.6 PURRR", " 4 Functional Programming 4.1 Iteration library(tidyverse) # library(here) # here() # hitters &lt;- read_csv(&quot;_bookdown_files/data_sources/Batting.csv&quot;, guess_max = 10000) hitters &lt;- read_csv(&quot;data_sources/Batting.csv&quot;, guess_max = 10000) ## Parsed with column specification: ## cols( ## .default = col_double(), ## playerID = col_character(), ## teamID = col_character(), ## lgID = col_character() ## ) ## See spec(...) for full column specifications. # hitters &lt;- read_csv(here(&quot;_bookdown_files/data_sources&quot;,&quot;Batting.csv&quot;), guess_max = 10000) # hitters &lt;- read_csv(&quot;data_sources/Batting.csv&quot;, guess_max = 10000) %&gt;% # janitor::clean_names() # hitters 4.2 An Interesting Question Who played the most games and hit the most Home Runs in the 90s in Texas? hitters %&gt;% filter(yearID %in% 1990:1999) %&gt;% filter(teamID %in% c(&quot;HOU&quot;,&quot;TEX&quot;)) %&gt;% select(playerID,teamID, G,HR) %&gt;% group_by(playerID, teamID) %&gt;% summarise_all(.funs = sum) %&gt;% arrange(desc(G,HR)) ## # A tibble: 411 x 4 ## # Groups: playerID [398] ## playerID teamID G HR ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 biggicr01 HOU 1515 136 ## 2 bagweje01 HOU 1317 263 ## 3 gonzaju03 TEX 1224 339 ## 4 rodriiv01 TEX 1169 144 ## 5 greerru01 TEX 809 103 ## 6 palmera01 TEX 790 146 ## 7 caminke01 HOU 772 74 ## 8 palmede01 TEX 758 154 ## 9 gonzalu01 HOU 745 62 ## 10 bellde01 HOU 683 74 ## # ... with 401 more rows 4.3 A More General Question Who, from any team over any number of years in any subset of tams had the most of any statistical category? min_year &lt;- 1990 max_year &lt;- 1999 teamID &lt;- c(&quot;HOU&quot;,&quot;TEX&quot;) category &lt;- c(&quot;G&quot;, &quot;HR&quot;) hitters %&gt;% filter(yearID %in% min_year:max_year) %&gt;% filter(teamID %in% !!enquo(teamID)) %&gt;% select(playerID, teamID, category) %&gt;% group_by(playerID, teamID) %&gt;% summarise_all(.funs = sum) %&gt;% arrange_at(desc({{ category }})) ## Note: Using an external vector in selections is ambiguous. ## i Use `all_of(category)` instead of `category` to silence this message. ## i See &lt;https://tidyselect.r-lib.org/reference/faq-external-vector.html&gt;. ## This message is displayed once per session. ## # A tibble: 411 x 4 ## # Groups: playerID [398] ## playerID teamID G HR ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 campbmi01 TEX 1 0 ## 2 davisdo01 TEX 1 0 ## 3 fernasi01 HOU 1 0 ## 4 grzanmi01 HOU 1 0 ## 5 jacksch03 TEX 1 0 ## 6 leeco01 TEX 1 0 ## 7 manonra01 TEX 1 0 ## 8 sassero01 TEX 1 0 ## 9 armstja01 TEX 2 0 ## 10 balbost01 TEX 2 0 ## # ... with 401 more rows var_quo = rlang::parse_quosures(category) ## Warning: `parse_quosures()` is deprecated as of rlang 0.2.0. ## Please use `parse_quos()` instead. ## This warning is displayed once per session. library(dplyr) library(rlang) test_func = function(var){ var_quo = parse_quosure(var) mtcars %&gt;% select(!!var_quo) %&gt;% arrange(!!var_quo) } test_func2 = function(var){ var_quo = enquo(var) mtcars %&gt;% select(!!var_quo) %&gt;% arrange(!!var_quo) } 4.4 Even more Genarally min_year &lt;- 1990 max_year &lt;- 1999 teamID &lt;- c(&quot;HOU&quot;,&quot;TEX&quot;) category &lt;- c(&quot;G&quot;, &quot;HR&quot;) baseball_fun &lt;- function(min_year = 1990, max_year = 1999, teamID = c(&quot;HOU&quot;,&quot;TEX&quot;), category = c(&quot;G&quot;, &quot;HR&quot;)){ hitters %&gt;% filter(yearID %in% min_year:max_year) %&gt;% filter(teamID %in% !!enquo(teamID)) %&gt;% select(playerID,teamID, category) %&gt;% group_by(playerID, teamID) %&gt;% summarise_all(.funs = sum) %&gt;% arrange_at(desc(category)) } baseball_fun() ## # A tibble: 411 x 4 ## # Groups: playerID [398] ## playerID teamID G HR ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 campbmi01 TEX 1 0 ## 2 davisdo01 TEX 1 0 ## 3 fernasi01 HOU 1 0 ## 4 grzanmi01 HOU 1 0 ## 5 jacksch03 TEX 1 0 ## 6 leeco01 TEX 1 0 ## 7 manonra01 TEX 1 0 ## 8 sassero01 TEX 1 0 ## 9 armstja01 TEX 2 0 ## 10 balbost01 TEX 2 0 ## # ... with 401 more rows 4.5 Even more Genarally hitters$teamID %&gt;% unique() %&gt;% sort() ## [1] &quot;ALT&quot; &quot;ANA&quot; &quot;ARI&quot; &quot;ATL&quot; &quot;BAL&quot; &quot;BFN&quot; &quot;BFP&quot; &quot;BL1&quot; &quot;BL2&quot; &quot;BL3&quot; &quot;BL4&quot; &quot;BLA&quot; ## [13] &quot;BLF&quot; &quot;BLN&quot; &quot;BLU&quot; &quot;BOS&quot; &quot;BR1&quot; &quot;BR2&quot; &quot;BR3&quot; &quot;BR4&quot; &quot;BRF&quot; &quot;BRO&quot; &quot;BRP&quot; &quot;BS1&quot; ## [25] &quot;BS2&quot; &quot;BSN&quot; &quot;BSP&quot; &quot;BSU&quot; &quot;BUF&quot; &quot;CAL&quot; &quot;CH1&quot; &quot;CH2&quot; &quot;CHA&quot; &quot;CHF&quot; &quot;CHN&quot; &quot;CHP&quot; ## [37] &quot;CHU&quot; &quot;CIN&quot; &quot;CL1&quot; &quot;CL2&quot; &quot;CL3&quot; &quot;CL4&quot; &quot;CL5&quot; &quot;CL6&quot; &quot;CLE&quot; &quot;CLP&quot; &quot;CN1&quot; &quot;CN2&quot; ## [49] &quot;CN3&quot; &quot;CNU&quot; &quot;COL&quot; &quot;DET&quot; &quot;DTN&quot; &quot;ELI&quot; &quot;FLO&quot; &quot;FW1&quot; &quot;HAR&quot; &quot;HOU&quot; &quot;HR1&quot; &quot;IN1&quot; ## [61] &quot;IN2&quot; &quot;IN3&quot; &quot;IND&quot; &quot;KC1&quot; &quot;KC2&quot; &quot;KCA&quot; &quot;KCF&quot; &quot;KCN&quot; &quot;KCU&quot; &quot;KEO&quot; &quot;LAA&quot; &quot;LAN&quot; ## [73] &quot;LS1&quot; &quot;LS2&quot; &quot;LS3&quot; &quot;MIA&quot; &quot;MID&quot; &quot;MIL&quot; &quot;MIN&quot; &quot;ML1&quot; &quot;ML2&quot; &quot;ML3&quot; &quot;ML4&quot; &quot;MLA&quot; ## [85] &quot;MLU&quot; &quot;MON&quot; &quot;NEW&quot; &quot;NH1&quot; &quot;NY1&quot; &quot;NY2&quot; &quot;NY3&quot; &quot;NY4&quot; &quot;NYA&quot; &quot;NYN&quot; &quot;NYP&quot; &quot;OAK&quot; ## [97] &quot;PH1&quot; &quot;PH2&quot; &quot;PH3&quot; &quot;PH4&quot; &quot;PHA&quot; &quot;PHI&quot; &quot;PHN&quot; &quot;PHP&quot; &quot;PHU&quot; &quot;PIT&quot; &quot;PRO&quot; &quot;PT1&quot; ## [109] &quot;PTF&quot; &quot;PTP&quot; &quot;RC1&quot; &quot;RC2&quot; &quot;RIC&quot; &quot;SDN&quot; &quot;SE1&quot; &quot;SEA&quot; &quot;SFN&quot; &quot;SL1&quot; &quot;SL2&quot; &quot;SL3&quot; ## [121] &quot;SL4&quot; &quot;SL5&quot; &quot;SLA&quot; &quot;SLF&quot; &quot;SLN&quot; &quot;SLU&quot; &quot;SPU&quot; &quot;SR1&quot; &quot;SR2&quot; &quot;TBA&quot; &quot;TEX&quot; &quot;TL1&quot; ## [133] &quot;TL2&quot; &quot;TOR&quot; &quot;TRN&quot; &quot;TRO&quot; &quot;WAS&quot; &quot;WIL&quot; &quot;WOR&quot; &quot;WS1&quot; &quot;WS2&quot; &quot;WS3&quot; &quot;WS4&quot; &quot;WS5&quot; ## [145] &quot;WS6&quot; &quot;WS7&quot; &quot;WS8&quot; &quot;WS9&quot; &quot;WSU&quot; baseball_fun(min_year = 2000,max_year = 2009, teamID = &quot;NYA&quot;) %&gt;% arrange(desc(HR)) ## # A tibble: 247 x 4 ## # Groups: playerID [247] ## playerID teamID G HR ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 rodrial01 NYA 891 238 ## 2 giambja01 NYA 897 209 ## 3 posadjo01 NYA 1302 208 ## 4 jeterde01 NYA 1500 161 ## 5 matsuhi01 NYA 916 140 ## 6 willibe02 NYA 980 136 ## 7 soriaal01 NYA 492 97 ## 8 canoro01 NYA 734 87 ## 9 damonjo01 NYA 576 77 ## 10 sheffga01 NYA 347 76 ## # ... with 237 more rows 4.6 PURRR library(purrr) map(c(&quot;TEX&quot;,&quot;NYA&quot;), ~baseball_fun(teamID = .)) %&gt;% map(~arrange(.x)) ## [[1]] ## # A tibble: 219 x 4 ## # Groups: playerID [219] ## playerID teamID G HR ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 campbmi01 TEX 1 0 ## 2 davisdo01 TEX 1 0 ## 3 jacksch03 TEX 1 0 ## 4 leeco01 TEX 1 0 ## 5 manonra01 TEX 1 0 ## 6 sassero01 TEX 1 0 ## 7 armstja01 TEX 2 0 ## 8 balbost01 TEX 2 0 ## 9 brumldu01 TEX 2 0 ## 10 caprani01 TEX 2 0 ## # ... with 209 more rows ## ## [[2]] ## # A tibble: 184 x 4 ## # Groups: playerID [184] ## playerID teamID G HR ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 lukema01 NYA 1 0 ## 2 judenje01 NYA 2 0 ## 3 ojedabo01 NYA 2 0 ## 4 riosda01 NYA 2 0 ## 5 whitewa02 NYA 2 0 ## 6 bruskji01 NYA 3 0 ## 7 chapida01 NYA 3 0 ## 8 harrigr01 NYA 3 0 ## 9 honeyri01 NYA 3 0 ## 10 jerzemi01 NYA 3 0 ## # ... with 174 more rows "],
["5-data-modeling.html", "5 Data Modeling 5.1 Load Packages 5.2 Read Data 5.3 Data Exploration 5.4 Split Data test/train 5.5 Preprocess Data 5.6 Apply Preprocessing 5.7 Prepare Cross Validation 5.8 Specify Models 5.9 Create Grid of Parameters to Validate Over 5.10 Execute Cross Validation 5.11 Select Best Parameters 5.12 Save Best Paramenters 5.13 Compare Models 5.14 Run Best Model on All Data 5.15 Run Model on New Data 5.16 Variable Importance", " 5 Data Modeling 5.1 Load Packages library(tidymodels) library(tune) library(dials) library(parsnip) library(rsample) library(recipes) library(textrecipes) library(yardstick) library(vip) library(gghighlight) library(patchwork) library(tidyverse) library(tidyquant) library(knitr) library(janitor) 5.2 Read Data data = read_csv(&quot;data_sources/Batting.csv&quot;, col_types = cols(SF = col_double(), GIDP = col_double())) %&gt;% clean_names() hofdata = read_csv(&quot;data_sources/HallOfFame.csv&quot;) %&gt;% clean_names() %&gt;% select(player_id, inducted) %&gt;% mutate(hof = ifelse(inducted==&quot;Y&quot;,1,0)) %&gt;% filter(hof==1) ## Parsed with column specification: ## cols( ## playerID = col_character(), ## yearid = col_double(), ## votedBy = col_character(), ## ballots = col_double(), ## needed = col_double(), ## votes = col_double(), ## inducted = col_character(), ## category = col_character(), ## needed_note = col_character() ## ) hofdata %&gt;% count(inducted) ## # A tibble: 1 x 2 ## inducted n ## &lt;chr&gt; &lt;int&gt; ## 1 Y 317 retiredyear = data %&gt;% group_by(player_id) %&gt;% summarise(lastyear = max(year_id)) ## `summarise()` ungrouping output (override with `.groups` argument) retiredyear ## # A tibble: 18,915 x 2 ## player_id lastyear ## &lt;chr&gt; &lt;dbl&gt; ## 1 aardsda01 2015 ## 2 aaronha01 1976 ## 3 aaronto01 1971 ## 4 aasedo01 1990 ## 5 abadan01 2006 ## 6 abadfe01 2016 ## 7 abadijo01 1875 ## 8 abbated01 1910 ## 9 abbeybe01 1896 ## 10 abbeych01 1897 ## # ... with 18,905 more rows hof = data %&gt;% group_by(player_id) %&gt;% summarise_at(vars(g:gidp), list(~sum(.,na.rm = TRUE))) %&gt;% ungroup() %&gt;% left_join(retiredyear) %&gt;% left_join(hofdata) %&gt;% mutate(inducted = if_else(inducted==&quot;Y&quot;,1,0)) %&gt;% mutate(inducted = replace_na(inducted, 0)) %&gt;% filter(g&gt;=1000) %&gt;% mutate(inducted = as.factor(inducted)) %&gt;% select(-hof) ## Joining, by = &quot;player_id&quot; ## Joining, by = &quot;player_id&quot; hof ## # A tibble: 1,565 x 20 ## player_id g ab r h x2b x3b hr rbi sb cs bb ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 aaronha01 3298 12364 2174 3771 624 98 755 2297 240 73 1402 ## 2 abreubo01 2425 8480 1453 2470 574 59 288 1363 400 128 1476 ## 3 adairje01 1165 4019 378 1022 163 19 57 366 29 29 208 ## 4 adamsbo03 1281 4019 591 1082 188 49 37 303 67 30 414 ## 5 adamssp01 1424 5557 844 1588 249 48 9 394 154 50 453 ## 6 adcocjo01 1959 6606 823 1832 295 35 336 1122 20 25 594 ## 7 ageeto01 1129 3912 558 999 170 27 130 433 167 81 342 ## 8 ainsmed01 1078 3048 299 707 108 54 22 317 86 16 263 ## 9 alfoned01 1506 5385 777 1532 282 18 146 744 53 17 596 ## 10 alicelu01 1341 3971 551 1031 189 53 47 422 81 50 500 ## # ... with 1,555 more rows, and 8 more variables: so &lt;dbl&gt;, ibb &lt;dbl&gt;, ## # hbp &lt;dbl&gt;, sh &lt;dbl&gt;, sf &lt;dbl&gt;, gidp &lt;dbl&gt;, lastyear &lt;dbl&gt;, inducted &lt;fct&gt; Filter for HOF Eligable Players hof %&gt;% filter(lastyear &lt;= 2012) %&gt;% count(inducted) ## # A tibble: 2 x 2 ## inducted n ## &lt;fct&gt; &lt;int&gt; ## 1 0 1229 ## 2 1 168 hofmod = hof %&gt;% filter(lastyear &lt;= 2012) %&gt;% select(-lastyear) hofmod ## # A tibble: 1,397 x 19 ## player_id g ab r h x2b x3b hr rbi sb cs bb ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 aaronha01 3298 12364 2174 3771 624 98 755 2297 240 73 1402 ## 2 adairje01 1165 4019 378 1022 163 19 57 366 29 29 208 ## 3 adamsbo03 1281 4019 591 1082 188 49 37 303 67 30 414 ## 4 adamssp01 1424 5557 844 1588 249 48 9 394 154 50 453 ## 5 adcocjo01 1959 6606 823 1832 295 35 336 1122 20 25 594 ## 6 ageeto01 1129 3912 558 999 170 27 130 433 167 81 342 ## 7 ainsmed01 1078 3048 299 707 108 54 22 317 86 16 263 ## 8 alfoned01 1506 5385 777 1532 282 18 146 744 53 17 596 ## 9 alicelu01 1341 3971 551 1031 189 53 47 422 81 50 500 ## 10 allenbe01 1139 3404 357 815 140 21 73 351 13 16 370 ## # ... with 1,387 more rows, and 7 more variables: so &lt;dbl&gt;, ibb &lt;dbl&gt;, ## # hbp &lt;dbl&gt;, sh &lt;dbl&gt;, sf &lt;dbl&gt;, gidp &lt;dbl&gt;, inducted &lt;fct&gt; hoftest = hof %&gt;% filter(lastyear &gt; 2012) %&gt;% select(-lastyear,-inducted) hoftest ## # A tibble: 168 x 18 ## player_id g ab r h x2b x3b hr rbi sb cs bb ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 abreubo01 2425 8480 1453 2470 574 59 288 1363 400 128 1476 ## 2 andruel01 1221 4625 648 1266 207 37 35 436 241 83 408 ## 3 aybarer01 1346 4842 606 1324 241 45 51 451 144 58 257 ## 4 barmecl01 1186 3805 434 932 208 17 89 415 43 30 216 ## 5 bautijo02 1519 5139 878 1311 267 17 308 862 60 26 881 ## 6 bayja01 1278 4505 737 1200 240 30 222 754 95 17 636 ## 7 beltrad01 2720 10295 1428 2942 591 36 445 1571 119 42 775 ## 8 beltrca01 2457 9301 1522 2617 536 78 421 1536 312 49 1051 ## 9 berkmla01 1879 6491 1146 1905 422 30 366 1234 86 48 1201 ## 10 betanyu01 1156 4052 437 1057 218 29 80 457 30 30 143 ## # ... with 158 more rows, and 6 more variables: so &lt;dbl&gt;, ibb &lt;dbl&gt;, hbp &lt;dbl&gt;, ## # sh &lt;dbl&gt;, sf &lt;dbl&gt;, gidp &lt;dbl&gt; 5.3 Data Exploration hofmod %&gt;% select(g:inducted) %&gt;% pivot_longer(cols = g:gidp) %&gt;% ggplot(aes(x=value,y=as.factor(inducted),color = name)) + geom_point() + theme(legend.position = &quot;none&quot;) + facet_wrap(~name, scales = &quot;free&quot;) 5.4 Split Data test/train set.seed(as.numeric(as.factor(&quot;beatnavy&quot;))) hof_initial_split = initial_split(hofmod, prop = 0.80) hof_initial_split ## &lt;Analysis/Assess/Total&gt; ## &lt;1118/279/1397&gt; 5.5 Preprocess Data preprocessing_recipe = recipe(inducted ~ ., data = training(hof_initial_split)) %&gt;% step_knnimpute(all_numeric()) %&gt;% step_center(all_numeric()) %&gt;% step_scale(all_numeric()) %&gt;% step_rm(player_id) %&gt;% prep() preprocessing_recipe ## Data Recipe ## ## Inputs: ## ## role #variables ## outcome 1 ## predictor 18 ## ## Training data contained 1118 data points and no missing data. ## ## Operations: ## ## K-nearest neighbor imputation for player_id, ab, r, h, x2b, x3b, hr, rbi, sb, ... [trained] ## Centering for g, ab, r, h, x2b, x3b, hr, rbi, sb, cs, bb, ... [trained] ## Scaling for g, ab, r, h, x2b, x3b, hr, rbi, sb, cs, bb, ... [trained] ## Variables removed player_id [trained] 5.6 Apply Preprocessing hof_training_preprocessed_tbl = preprocessing_recipe %&gt;% bake(training(hof_initial_split)) hof_training_preprocessed_tbl ## # A tibble: 1,118 x 18 ## g ab r h x2b x3b hr rbi sb cs ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 3.96 3.72 3.93 3.84 3.27 1.08e+0 5.51 4.55 0.786 0.783 ## 2 -0.873 -0.702 -1.08 -0.770 -0.790 -8.61e-1 -0.556 -0.939 -0.701 -0.323 ## 3 -0.610 -0.702 -0.489 -0.669 -0.570 -1.22e-1 -0.730 -1.12 -0.433 -0.298 ## 4 -0.286 0.113 0.217 0.179 -0.0318 -1.47e-1 -0.974 -0.859 0.180 0.205 ## 5 0.925 0.670 0.158 0.589 0.374 -4.67e-1 1.87 1.21 -0.764 -0.423 ## 6 -0.954 -0.759 -0.582 -0.809 -0.728 -6.64e-1 0.0781 -0.748 0.272 0.984 ## 7 -1.07 -1.22 -1.30 -1.30 -1.27 5.94e-4 -0.861 -1.08 -0.299 -0.649 ## 8 -0.101 0.0222 0.0300 0.0853 0.259 -8.85e-1 0.217 0.136 -0.532 -0.624 ## 9 -0.474 -0.728 -0.601 -0.755 -0.561 -2.40e-2 -0.643 -0.780 -0.334 0.205 ## 10 -0.932 -1.03 -1.14 -1.12 -0.993 -8.12e-1 -0.417 -0.981 -0.813 -0.649 ## # ... with 1,108 more rows, and 8 more variables: bb &lt;dbl&gt;, so &lt;dbl&gt;, ## # ibb &lt;dbl&gt;, hbp &lt;dbl&gt;, sh &lt;dbl&gt;, sf &lt;dbl&gt;, gidp &lt;dbl&gt;, inducted &lt;fct&gt; 5.7 Prepare Cross Validation set.seed(as.numeric(as.factor(&quot;beatnavy&quot;))) hof_cv_folds = training(hof_initial_split) %&gt;% bake(preprocessing_recipe, new_data = .) %&gt;% vfold_cv(v = 5) hof_cv_folds ## # 5-fold cross-validation ## # A tibble: 5 x 2 ## splits id ## &lt;list&gt; &lt;chr&gt; ## 1 &lt;split [894/224]&gt; Fold1 ## 2 &lt;split [894/224]&gt; Fold2 ## 3 &lt;split [894/224]&gt; Fold3 ## 4 &lt;split [895/223]&gt; Fold4 ## 5 &lt;split [895/223]&gt; Fold5 5.8 Specify Models GLM Model glmnet_model = logistic_reg(mode = &quot;classification&quot;, penalty = tune(), mixture = tune()) %&gt;% set_engine(&quot;glmnet&quot;) glmnet_model ## Logistic Regression Model Specification (classification) ## ## Main Arguments: ## penalty = tune() ## mixture = tune() ## ## Computational engine: glmnet Random Forest Model forest_model = rand_forest( mode = &quot;classification&quot;, mtry = tune(), trees = tune(), min_n = tune() ) %&gt;% set_engine(&quot;randomForest&quot;, objective = &quot;reg:squarederror&quot;) forest_model ## Random Forest Model Specification (classification) ## ## Main Arguments: ## mtry = tune() ## trees = tune() ## min_n = tune() ## ## Engine-Specific Arguments: ## objective = reg:squarederror ## ## Computational engine: randomForest 5.9 Create Grid of Parameters to Validate Over GLM Model glmnet_params = parameters(penalty(), mixture()) glmnet_params ## Collection of 2 parameters for tuning ## ## id parameter type object class ## penalty penalty nparam[+] ## mixture mixture nparam[+] set.seed(as.numeric(as.factor(&quot;beatnavy&quot;))) glmnet_grid = grid_max_entropy(glmnet_params, size = 20) glmnet_grid ## # A tibble: 20 x 2 ## penalty mixture ## &lt;dbl&gt; &lt;dbl&gt; ## 1 1.77e- 9 0.508 ## 2 2.24e- 2 0.0442 ## 3 4.54e-10 0.852 ## 4 1.93e- 8 0.206 ## 5 5.17e- 2 0.988 ## 6 4.62e- 9 0.0242 ## 7 1.94e- 6 0.338 ## 8 8.83e- 1 0.608 ## 9 8.52e- 3 0.597 ## 10 1.23e-10 0.105 ## 11 2.40e-10 0.313 ## 12 5.91e- 8 0.694 ## 13 8.03e- 8 0.936 ## 14 1.32e- 2 0.350 ## 15 8.91e- 6 0.515 ## 16 1.66e- 4 0.915 ## 17 1.44e- 4 0.702 ## 18 1.38e- 6 0.0159 ## 19 1.35e-10 0.643 ## 20 7.04e- 4 0.196 glmnet_grid %&gt;% ggplot(aes(penalty, mixture)) + geom_point(size = 3) + scale_x_log10() + labs(title = &quot;Max Entropy Grid&quot;, x = &quot;Penalty (log scale)&quot;, y = &quot;Mixture&quot;) Random Forest Model forest_params = parameters(mtry(c(2,6)), trees(), min_n()) forest_params ## Collection of 3 parameters for tuning ## ## id parameter type object class ## mtry mtry nparam[+] ## trees trees nparam[+] ## min_n min_n nparam[+] set.seed(as.numeric(as.factor(&quot;beatnavy&quot;))) forest_grid = grid_max_entropy(forest_params, size = 30) forest_grid ## # A tibble: 30 x 3 ## mtry trees min_n ## &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 2 1264 5 ## 2 4 1113 28 ## 3 5 622 11 ## 4 2 28 6 ## 5 3 24 19 ## 6 3 1479 36 ## 7 4 1404 13 ## 8 5 33 40 ## 9 6 115 15 ## 10 6 686 3 ## # ... with 20 more rows 5.10 Execute Cross Validation 5.10.1 Parallel Processing all_cores &lt;- parallel::detectCores(logical = FALSE) library(doFuture) registerDoFuture() cl &lt;- makeCluster(all_cores) plan(cluster, workers = cl) GLM Model library(tune) glmnet_stage_1_cv_results_tbl = tune_grid( formula = inducted ~ ., model = glmnet_model, resamples = hof_cv_folds, grid = glmnet_grid, metrics = metric_set(accuracy, kap, roc_auc), control = control_grid(verbose = TRUE) ) glmnet_stage_1_cv_results_tbl %&gt;% show_best(&quot;accuracy&quot;, n = 10, maximize = FALSE) ## # A tibble: 10 x 7 ## penalty mixture .metric .estimator mean n std_err ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 8.52e- 3 0.597 accuracy binary 0.920 5 0.0117 ## 2 2.24e- 2 0.0442 accuracy binary 0.918 5 0.0113 ## 3 1.32e- 2 0.350 accuracy binary 0.917 5 0.0109 ## 4 1.38e- 6 0.0159 accuracy binary 0.914 5 0.0115 ## 5 4.62e- 9 0.0242 accuracy binary 0.913 5 0.0121 ## 6 5.17e- 2 0.988 accuracy binary 0.913 5 0.00871 ## 7 1.23e-10 0.105 accuracy binary 0.913 5 0.0105 ## 8 1.35e-10 0.643 accuracy binary 0.913 5 0.00990 ## 9 4.54e-10 0.852 accuracy binary 0.913 5 0.00990 ## 10 1.77e- 9 0.508 accuracy binary 0.913 5 0.0107 glmnet_stage_1_cv_results_tbl %&gt;% show_best(&quot;kap&quot;, n = 10, maximize = FALSE) ## # A tibble: 10 x 7 ## penalty mixture .metric .estimator mean n std_err ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 8.52e- 3 0.597 kap binary 0.518 5 0.0627 ## 2 1.23e-10 0.105 kap binary 0.507 5 0.0454 ## 3 1.35e-10 0.643 kap binary 0.506 5 0.0423 ## 4 4.54e-10 0.852 kap binary 0.506 5 0.0423 ## 5 5.91e- 8 0.694 kap binary 0.506 5 0.0423 ## 6 8.03e- 8 0.936 kap binary 0.506 5 0.0423 ## 7 1.77e- 9 0.508 kap binary 0.504 5 0.0442 ## 8 8.91e- 6 0.515 kap binary 0.504 5 0.0442 ## 9 1.38e- 6 0.0159 kap binary 0.502 5 0.0530 ## 10 2.40e-10 0.313 kap binary 0.500 5 0.0432 glmnet_stage_1_cv_results_tbl %&gt;% show_best(&quot;roc_auc&quot;, n = 10, maximize = FALSE) ## # A tibble: 10 x 7 ## penalty mixture .metric .estimator mean n std_err ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 7.04e- 4 0.196 roc_auc binary 0.922 5 0.0116 ## 2 1.38e- 6 0.0159 roc_auc binary 0.921 5 0.0118 ## 3 4.62e- 9 0.0242 roc_auc binary 0.921 5 0.0117 ## 4 8.52e- 3 0.597 roc_auc binary 0.921 5 0.0128 ## 5 1.23e-10 0.105 roc_auc binary 0.921 5 0.0118 ## 6 1.44e- 4 0.702 roc_auc binary 0.920 5 0.0119 ## 7 1.93e- 8 0.206 roc_auc binary 0.920 5 0.0119 ## 8 1.32e- 2 0.350 roc_auc binary 0.920 5 0.0128 ## 9 2.40e-10 0.313 roc_auc binary 0.920 5 0.0119 ## 10 1.66e- 4 0.915 roc_auc binary 0.919 5 0.0120 Random Forest Model forest_stage_1_cv_results_tbl = tune_grid( formula = inducted ~ ., model = forest_model, resamples = hof_cv_folds, grid = forest_grid, metrics = metric_set(accuracy, kap, roc_auc), control = control_grid(verbose = TRUE) ) forest_stage_1_cv_results_tbl %&gt;% show_best(&quot;accuracy&quot;, n = 10, maximize = FALSE) ## # A tibble: 10 x 8 ## mtry trees min_n .metric .estimator mean n std_err ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 4 1849 40 accuracy binary 0.922 5 0.0122 ## 2 2 28 6 accuracy binary 0.921 5 0.0117 ## 3 3 379 31 accuracy binary 0.921 5 0.0102 ## 4 5 1813 24 accuracy binary 0.921 5 0.0102 ## 5 5 622 11 accuracy binary 0.920 5 0.00997 ## 6 5 1130 17 accuracy binary 0.920 5 0.00947 ## 7 6 663 38 accuracy binary 0.920 5 0.0110 ## 8 6 1508 35 accuracy binary 0.920 5 0.0110 ## 9 4 1113 28 accuracy binary 0.920 5 0.00905 ## 10 4 725 39 accuracy binary 0.920 5 0.0128 forest_stage_1_cv_results_tbl %&gt;% show_best(&quot;kap&quot;, n = 10, maximize = FALSE) ## # A tibble: 10 x 8 ## mtry trees min_n .metric .estimator mean n std_err ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 2 28 6 kap binary 0.541 5 0.0747 ## 2 4 1849 40 kap binary 0.537 5 0.0845 ## 3 3 379 31 kap binary 0.537 5 0.0716 ## 4 5 622 11 kap binary 0.533 5 0.0608 ## 5 5 1813 24 kap binary 0.533 5 0.0757 ## 6 3 292 12 kap binary 0.526 5 0.0625 ## 7 2 824 38 kap binary 0.526 5 0.0648 ## 8 5 33 40 kap binary 0.525 5 0.0709 ## 9 5 1130 17 kap binary 0.525 5 0.0692 ## 10 4 1113 28 kap binary 0.525 5 0.0686 forest_stage_1_cv_results_tbl %&gt;% show_best(&quot;roc_auc&quot;, n = 10, maximize = FALSE) ## # A tibble: 10 x 8 ## mtry trees min_n .metric .estimator mean n std_err ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 2 605 13 roc_auc binary 0.926 5 0.0137 ## 2 2 1264 5 roc_auc binary 0.925 5 0.0124 ## 3 3 377 3 roc_auc binary 0.923 5 0.0143 ## 4 3 1479 36 roc_auc binary 0.922 5 0.0135 ## 5 2 1986 16 roc_auc binary 0.922 5 0.0132 ## 6 4 1128 2 roc_auc binary 0.922 5 0.0141 ## 7 6 686 3 roc_auc binary 0.921 5 0.0141 ## 8 3 1963 26 roc_auc binary 0.921 5 0.0140 ## 9 6 1508 35 roc_auc binary 0.920 5 0.0155 ## 10 3 963 39 roc_auc binary 0.920 5 0.0158 5.11 Select Best Parameters params_glmnet_best = glmnet_stage_1_cv_results_tbl %&gt;% select_best(&quot;roc_auc&quot;, maximize = FALSE) ## Warning: The `maximize` argument is no longer needed. This value was ignored. params_glmnet_best ## # A tibble: 1 x 2 ## penalty mixture ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0.000704 0.196 params_forest_best = forest_stage_1_cv_results_tbl %&gt;% select_best(&quot;roc_auc&quot;, maximize = FALSE) ## Warning: The `maximize` argument is no longer needed. This value was ignored. params_forest_best ## # A tibble: 1 x 3 ## mtry trees min_n ## &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 2 605 13 5.12 Save Best Paramenters glmnet_stage_2_model = glmnet_model %&gt;% finalize_model(parameters = params_glmnet_best) glmnet_stage_2_model ## Logistic Regression Model Specification (classification) ## ## Main Arguments: ## penalty = 0.000703590468650661 ## mixture = 0.195607561385259 ## ## Computational engine: glmnet forest_stage_2_model = forest_model %&gt;% finalize_model(params_forest_best) forest_stage_2_model ## Random Forest Model Specification (classification) ## ## Main Arguments: ## mtry = 2 ## trees = 605 ## min_n = 13 ## ## Engine-Specific Arguments: ## objective = reg:squarederror ## ## Computational engine: randomForest 5.13 Compare Models train_processed = training(hof_initial_split) %&gt;% bake(preprocessing_recipe, new_data = .) test_processed = testing(hof_initial_split) %&gt;% bake(preprocessing_recipe, new_data = .) target_expr = preprocessing_recipe %&gt;% pluck(&quot;last_term_info&quot;) %&gt;% filter(role == &quot;outcome&quot;) %&gt;% pull(variable) %&gt;% sym() glmnet_stage_2_metrics = glmnet_stage_2_model %&gt;% fit(formula = inducted ~ ., data = train_processed) %&gt;% predict(new_data = test_processed) %&gt;% bind_cols(testing(hof_initial_split)) %&gt;% metrics(!! target_expr, .pred_class) forest_stage_2_metrics = forest_stage_2_model %&gt;% fit(formula = inducted ~ ., data = train_processed) %&gt;% predict(new_data = test_processed) %&gt;% bind_cols(testing(hof_initial_split)) %&gt;% metrics(!! target_expr, .pred_class) glmnet_stage_2_metrics %&gt;% mutate(mod = &quot;glmnet&quot;) %&gt;% bind_rows( forest_stage_2_metrics %&gt;% mutate(mod = &quot;forest&quot;) ) %&gt;% arrange(.metric,-.estimate) ## # A tibble: 4 x 4 ## .metric .estimator .estimate mod ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 accuracy binary 0.907 glmnet ## 2 accuracy binary 0.892 forest ## 3 kap binary 0.521 glmnet ## 4 kap binary 0.461 forest Looks like the Random Forest is the better model. 5.14 Run Best Model on All Data model_final = forest_stage_2_model %&gt;% fit(inducted ~ . , data = bake(preprocessing_recipe, new_data = hofmod)) 5.15 Run Model on New Data hoftest %&gt;% bake(preprocessing_recipe, new_data = .) %&gt;% predict(model_final, new_data = .) %&gt;% bind_cols(hoftest %&gt;% select(player_id)) %&gt;% arrange(desc(.pred_class)) ## # A tibble: 168 x 2 ## .pred_class player_id ## &lt;fct&gt; &lt;chr&gt; ## 1 1 beltrad01 ## 2 1 beltrca01 ## 3 1 jeterde01 ## 4 1 pujolal01 ## 5 1 rodrial01 ## 6 1 rolliji01 ## 7 1 suzukic01 ## 8 0 abreubo01 ## 9 0 andruel01 ## 10 0 aybarer01 ## # ... with 158 more rows 5.16 Variable Importance vip(model_final) + labs(title = &quot;Random Forest Model Importance - HOF Prediction&quot;) Many Thanks "],
["6-text-analysis.html", "6 Text Analysis 6.1 The Adventures of Tom Sawyer 6.2 Find Chapter Splits 6.3 Tokenize the Book 6.4 Remove ‘stop words’ 6.5 Join Sentiments 6.6 Descriptive Text Statistics 6.7 Visualizations 6.8 N-Gram Analysis 6.9 Term Frequency 6.10 Topic Modeling", " 6 Text Analysis 6.1 The Adventures of Tom Sawyer library(tidyverse) library(tidytext) library(stringi) book = read_file(&quot;data_sources//The-Adventures-of-Tom-Sawyer.txt&quot;) %&gt;% enframe(name = &quot;Book&quot;) # book = read_file(&quot;_bookdown_files/data_sources/The-Adventures-of-Tom-Sawyer.txt&quot;) %&gt;% enframe(name = &quot;Book&quot;) book ## # A tibble: 1 x 2 ## Book value ## &lt;int&gt; &lt;chr&gt; ## 1 1 &quot;The Project Gutenberg EBook of The Adventures of Tom Sawyer, Complete\\~ book %&gt;% nchar() ## Book value ## 1 423754 6.2 Find Chapter Splits book = book %&gt;% separate_rows(value, sep = &quot;\\nCHAPTER&quot;) %&gt;% slice(-1) %&gt;% mutate(value = str_remove_all(string = value, pattern = &quot;\\n&quot;)) %&gt;% mutate(value = str_replace(value, &quot;jpg&quot;, &quot;HERE&quot;)) %&gt;% separate(col = &quot;value&quot;, into = c(&quot;Chapter&quot;, &quot;Text&quot;), sep = &quot;HERE&quot;) %&gt;% filter(!is.na(Text)) %&gt;% mutate(Chapter = unlist(str_extract_all(Chapter, &quot;[A-Z]+&quot;))) %&gt;% mutate(Text = str_replace_all(Text, &quot;[.]&quot;,&quot; &quot;)) %&gt;% mutate(Chapter = as.numeric(as.roman(Chapter))) ## Warning: Expected 2 pieces. Additional pieces discarded in 2 rows [55, 60]. ## Warning: Expected 2 pieces. Missing pieces filled with `NA` in 35 rows [1, 2, 3, ## 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, ...]. book ## # A tibble: 35 x 3 ## Book Chapter Text ## &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 1 1 &quot; (182K)\\r\\r\\r\\r\\r\\r“TOM!”\\r\\rNo answer \\r\\r“TOM!”\\r\\rNo answe~ ## 2 1 2 &quot; (202K)\\r\\r\\r\\r\\r\\rSATURDAY morning was come, and all the sum~ ## 3 1 3 &quot; (197K)\\r\\r\\r\\r\\r\\rTOM presented himself before Aunt Polly, w~ ## 4 1 4 &quot; (218K)\\r\\r\\r\\r\\r\\rTHE sun rose upon a tranquil world, and be~ ## 5 1 5 &quot; (205K)\\r\\r\\r\\r\\r\\rABOUT half-past ten the cracked bell of th~ ## 6 1 6 &quot; (202K)\\r\\r\\r\\r\\r\\rMONDAY morning found Tom Sawyer miserable ~ ## 7 1 7 &quot; (175K)\\r\\r\\r\\r\\r\\rTHE harder Tom tried to fasten his mind on~ ## 8 1 8 &quot; (195K)\\r\\r\\r\\r\\r\\rTOM dodged hither and thither through lane~ ## 9 1 9 &quot; (174K)\\r\\r\\r\\r\\r\\rAT half-past nine, that night, Tom and Sid~ ## 10 1 10 &quot; (171K)\\r\\r\\r\\r\\r\\rTHE two boys flew on and on, toward the vi~ ## # ... with 25 more rows 6.3 Tokenize the Book booktokens = book %&gt;% unnest_tokens(word, Text) booktokens ## # A tibble: 70,882 x 3 ## Book Chapter word ## &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 1 1 182k ## 2 1 1 tom ## 3 1 1 no ## 4 1 1 answer ## 5 1 1 tom ## 6 1 1 no ## 7 1 1 answer ## 8 1 1 what’s ## 9 1 1 gone ## 10 1 1 with ## # ... with 70,872 more rows 6.4 Remove ‘stop words’ bookstop = booktokens %&gt;% anti_join(stop_words) ## Joining, by = &quot;word&quot; bookstop ## # A tibble: 26,251 x 3 ## Book Chapter word ## &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 1 1 182k ## 2 1 1 tom ## 3 1 1 answer ## 4 1 1 tom ## 5 1 1 answer ## 6 1 1 what’s ## 7 1 1 boy ## 8 1 1 tom ## 9 1 1 answer ## 10 1 1 lady ## # ... with 26,241 more rows 6.5 Join Sentiments get_sentiments(lexicon = &quot;afinn&quot;) ## # A tibble: 2,477 x 2 ## word value ## &lt;chr&gt; &lt;dbl&gt; ## 1 abandon -2 ## 2 abandoned -2 ## 3 abandons -2 ## 4 abducted -2 ## 5 abduction -2 ## 6 abductions -2 ## 7 abhor -3 ## 8 abhorred -3 ## 9 abhorrent -3 ## 10 abhors -3 ## # ... with 2,467 more rows get_sentiments(lexicon = &quot;bing&quot;) ## # A tibble: 6,786 x 2 ## word sentiment ## &lt;chr&gt; &lt;chr&gt; ## 1 2-faces negative ## 2 abnormal negative ## 3 abolish negative ## 4 abominable negative ## 5 abominably negative ## 6 abominate negative ## 7 abomination negative ## 8 abort negative ## 9 aborted negative ## 10 aborts negative ## # ... with 6,776 more rows get_sentiments(lexicon = &quot;loughran&quot;) ## # A tibble: 4,150 x 2 ## word sentiment ## &lt;chr&gt; &lt;chr&gt; ## 1 abandon negative ## 2 abandoned negative ## 3 abandoning negative ## 4 abandonment negative ## 5 abandonments negative ## 6 abandons negative ## 7 abdicated negative ## 8 abdicates negative ## 9 abdicating negative ## 10 abdication negative ## # ... with 4,140 more rows get_sentiments(lexicon = &quot;nrc&quot;) ## # A tibble: 13,901 x 2 ## word sentiment ## &lt;chr&gt; &lt;chr&gt; ## 1 abacus trust ## 2 abandon fear ## 3 abandon negative ## 4 abandon sadness ## 5 abandoned anger ## 6 abandoned fear ## 7 abandoned negative ## 8 abandoned sadness ## 9 abandonment anger ## 10 abandonment fear ## # ... with 13,891 more rows booktokens %&gt;% left_join(get_sentiments(&quot;bing&quot;)) ## Joining, by = &quot;word&quot; ## # A tibble: 70,882 x 4 ## Book Chapter word sentiment ## &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1 1 182k &lt;NA&gt; ## 2 1 1 tom &lt;NA&gt; ## 3 1 1 no &lt;NA&gt; ## 4 1 1 answer &lt;NA&gt; ## 5 1 1 tom &lt;NA&gt; ## 6 1 1 no &lt;NA&gt; ## 7 1 1 answer &lt;NA&gt; ## 8 1 1 what’s &lt;NA&gt; ## 9 1 1 gone &lt;NA&gt; ## 10 1 1 with &lt;NA&gt; ## # ... with 70,872 more rows booktokens %&gt;% left_join(get_sentiments(&quot;bing&quot;)) %&gt;% filter(!is.na(sentiment)) ## Joining, by = &quot;word&quot; ## # A tibble: 4,778 x 4 ## Book Chapter word sentiment ## &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1 1 wonder positive ## 2 1 1 pride positive ## 3 1 1 well positive ## 4 1 1 perplexed negative ## 5 1 1 loud negative ## 6 1 1 enough positive ## 7 1 1 well positive ## 8 1 1 noise negative ## 9 1 1 slack negative ## 10 1 1 well positive ## # ... with 4,768 more rows 6.6 Descriptive Text Statistics booktokens %&gt;% left_join(get_sentiments(&quot;bing&quot;)) %&gt;% filter(!is.na(sentiment)) %&gt;% count(Chapter,sentiment) ## Joining, by = &quot;word&quot; ## # A tibble: 64 x 3 ## Chapter sentiment n ## &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; ## 1 1 negative 95 ## 2 1 positive 81 ## 3 2 negative 40 ## 4 2 positive 66 ## 5 3 negative 96 ## 6 3 positive 84 ## 7 4 negative 96 ## 8 4 positive 147 ## 9 5 negative 63 ## 10 5 positive 60 ## # ... with 54 more rows 6.7 Visualizations booktokens %&gt;% left_join(get_sentiments(&quot;bing&quot;)) %&gt;% filter(!is.na(sentiment)) %&gt;% count(Chapter,sentiment) %&gt;% mutate(n = if_else(sentiment == &quot;negative&quot;,n*-1,as.double(n))) %&gt;% group_by(Chapter) %&gt;% mutate(order = group_indices()) %&gt;% summarise(n = sum(n)) %&gt;% mutate(pos = if_else(n&gt;0,&quot;pos&quot;,&quot;neg&quot;)) %&gt;% ungroup() %&gt;% ggplot(aes(x=Chapter,y=n,fill = pos, color = pos)) + geom_col() + scale_fill_manual(values = c(&quot;red&quot;,&quot;green&quot;)) + scale_color_manual(values = c(&quot;black&quot;,&quot;black&quot;)) + theme(legend.position=&quot;none&quot;, axis.text.x = element_text(angle = 90)) + labs(y = &quot;Net Positive Words&quot;, title = &quot;Sentiment Analysis of &#39;The Adventures of Tom Sawyer&#39;&quot;, subtitle = &quot;Net Positive Words by Chapter&quot;) ## Joining, by = &quot;word&quot; ## Warning: `group_indices()` is deprecated as of dplyr 1.0.0. ## Please use `cur_group_id()` instead. ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_warnings()` to see where this warning was generated. ## `summarise()` ungrouping output (override with `.groups` argument) 6.8 N-Gram Analysis 6.8.1 Uni-Grams booktokens %&gt;% count(word, sort = TRUE) ## # A tibble: 7,774 x 2 ## word n ## &lt;chr&gt; &lt;int&gt; ## 1 the 3708 ## 2 and 3059 ## 3 a 1807 ## 4 to 1696 ## 5 of 1474 ## 6 he 1158 ## 7 was 1126 ## 8 it 1090 ## 9 in 943 ## 10 that 875 ## # ... with 7,764 more rows 6.8.2 Remove Stop Words booktokens %&gt;% left_join(get_sentiments(&quot;bing&quot;)) %&gt;% filter(!is.na(sentiment)) %&gt;% count(word,sentiment, sort = TRUE) ## Joining, by = &quot;word&quot; ## # A tibble: 1,358 x 3 ## word sentiment n ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 well positive 152 ## 2 like positive 113 ## 3 good positive 101 ## 4 work positive 88 ## 5 right positive 83 ## 6 great positive 68 ## 7 dead negative 59 ## 8 enough positive 57 ## 9 poor negative 52 ## 10 cave negative 41 ## # ... with 1,348 more rows 6.8.3 Visualize booktokens %&gt;% left_join(get_sentiments(&quot;bing&quot;)) %&gt;% filter(!is.na(sentiment)) %&gt;% count(word, sentiment, sort = TRUE) %&gt;% group_by(sentiment) %&gt;% top_n(10) %&gt;% ungroup() %&gt;% ggplot(aes(x=fct_reorder(word,n), y = n, fill = sentiment)) + geom_col(show.legend = FALSE) + facet_wrap(~sentiment, scales = &quot;free_y&quot;) + coord_flip() + labs(x=&quot;Word&quot;) ## Joining, by = &quot;word&quot; ## Selecting by n 6.8.4 Bigrams bookbitokens = book %&gt;% unnest_tokens(bigram, Text, token = &quot;ngrams&quot;, n = 2, n_min = 2) bookbitokens ## # A tibble: 70,848 x 3 ## Book Chapter bigram ## &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 1 1 182k tom ## 2 1 1 tom no ## 3 1 1 no answer ## 4 1 1 answer tom ## 5 1 1 tom no ## 6 1 1 no answer ## 7 1 1 answer what’s ## 8 1 1 what’s gone ## 9 1 1 gone with ## 10 1 1 with that ## # ... with 70,838 more rows bookbitokens %&gt;% count(bigram, sort = TRUE) ## # A tibble: 41,080 x 2 ## bigram n ## &lt;chr&gt; &lt;int&gt; ## 1 of the 364 ## 2 in the 298 ## 3 and the 184 ## 4 it was 175 ## 5 to the 175 ## 6 he was 147 ## 7 and then 126 ## 8 was a 116 ## 9 he had 110 ## 10 there was 110 ## # ... with 41,070 more rows 6.8.5 Remove Stop Words in Bigrams bookbitokens %&gt;% separate(bigram, c(&quot;word1&quot;, &quot;word2&quot;), sep = &quot; &quot;) ## # A tibble: 70,848 x 4 ## Book Chapter word1 word2 ## &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1 1 182k tom ## 2 1 1 tom no ## 3 1 1 no answer ## 4 1 1 answer tom ## 5 1 1 tom no ## 6 1 1 no answer ## 7 1 1 answer what’s ## 8 1 1 what’s gone ## 9 1 1 gone with ## 10 1 1 with that ## # ... with 70,838 more rows bigrams = bookbitokens %&gt;% separate(bigram, c(&quot;word1&quot;, &quot;word2&quot;), sep = &quot; &quot;) %&gt;% filter(!word1 %in% stop_words$word) %&gt;% filter(!word2 %in% stop_words$word) bigrams %&gt;% count(word1, word2, sort = TRUE) ## # A tibble: 6,910 x 3 ## word1 word2 n ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 project gutenberg 84 ## 2 gutenberg tm 56 ## 3 injun joe 45 ## 4 aunt polly 42 ## 5 tom sawyer 23 ## 6 injun joe’s 18 ## 7 tm electronic 18 ## 8 muff potter 15 ## 9 archive foundation 13 ## 10 gutenberg literary 13 ## # ... with 6,900 more rows 6.8.6 Visualize Bigrams bigrams %&gt;% unite(col = &quot;bigram&quot;, word1,word2, sep = &quot; &quot;) %&gt;% count(bigram, sort = TRUE) %&gt;% top_n(20) %&gt;% ggplot(aes(x=fct_reorder(bigram,n),y = n)) + geom_col() + coord_flip() + labs(x=&quot;Bigram&quot;,y = &quot;Count&quot;, title = &quot;Top Bigrams&quot;) ## Selecting by n 6.9 Term Frequency Term Frequency: The number of times that a term occurs in the book. Inverse Document Frequency: \\(\\ln(\\frac{Total Number of Documents, cache = TRUE}{Total Number of Documents Containing Specified Word, cache = TRUE})\\): Measure of how much information the word provides. Term Frequency - Inverse Document Frequency: Term Frequency * Inverse Document Frequency 6.9.1 Build TF-IDF Data Words By Chapter booktokens %&gt;% count(Chapter, word, sort = TRUE, name = &quot;count&quot;) %&gt;% add_count(word) %&gt;% spread(Chapter, count) %&gt;% arrange(desc(n)) ## # A tibble: 7,774 x 36 ## word n `1` `2` `3` `4` `5` `6` `7` `8` `9` `10` `11` ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 a 32 66 75 62 132 58 94 34 53 54 52 29 ## 2 all 32 4 8 9 20 7 12 12 6 11 2 3 ## 3 and 32 102 77 110 166 101 139 96 88 99 87 66 ## 4 as 32 16 4 12 18 22 14 10 7 12 10 11 ## 5 be 32 4 5 4 15 7 3 6 11 9 4 5 ## 6 befo~ 32 3 3 4 2 2 3 4 3 2 3 6 ## 7 but 32 19 16 18 22 14 33 22 11 11 18 10 ## 8 for 32 26 11 18 46 27 15 9 6 14 10 12 ## 9 got 32 6 6 6 7 1 10 4 2 5 7 2 ## 10 had 32 13 13 22 27 15 13 8 17 9 8 17 ## # ... with 7,764 more rows, and 23 more variables: `12` &lt;int&gt;, `13` &lt;int&gt;, ## # `14` &lt;int&gt;, `15` &lt;int&gt;, `16` &lt;int&gt;, `17` &lt;int&gt;, `18` &lt;int&gt;, `19` &lt;int&gt;, ## # `20` &lt;int&gt;, `21` &lt;int&gt;, `22` &lt;int&gt;, `23` &lt;int&gt;, `24` &lt;int&gt;, `25` &lt;int&gt;, ## # `27` &lt;int&gt;, `28` &lt;int&gt;, `29` &lt;int&gt;, `30` &lt;int&gt;, `31` &lt;int&gt;, `32` &lt;int&gt;, ## # `33` &lt;int&gt;, `34` &lt;int&gt;, `35` &lt;int&gt; Word Frequency Per Chapter and Book booktokens %&gt;% count(Chapter, word, sort = TRUE, name = &quot;Chapter_Total&quot;) %&gt;% left_join( booktokens %&gt;% count(word, sort = TRUE, name = &quot;Book_Total&quot;) ) ## Joining, by = &quot;word&quot; ## # A tibble: 24,205 x 4 ## Chapter word Chapter_Total Book_Total ## &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; ## 1 16 the 275 3708 ## 2 16 and 269 3059 ## 3 35 the 245 3708 ## 4 33 the 205 3708 ## 5 30 the 187 3708 ## 6 4 the 185 3708 ## 7 5 the 167 3708 ## 8 4 and 166 3059 ## 9 29 the 166 3708 ## 10 21 the 162 3708 ## # ... with 24,195 more rows Create TF-IDF {, cache = TRUE} booktokens %&gt;% count(Chapter, word, sort = TRUE, name = \"Chapter_Total\") %&gt;% left_join( booktokens %&gt;% count(word, sort = TRUE, name = \"Book_Total\") ) %&gt;% bind_tf_idf(word, Chapter, Chapter_Total) %&gt;% filter(Chapter_Total!=Book_Total) %&gt;% filter(tf&lt;1) %&gt;% arrange(-tf_idf) 6.9.2 Visualize TF-IDF booktokens %&gt;% count(Chapter, word, sort = TRUE, name = &quot;Chapter_Total&quot;) %&gt;% left_join( booktokens %&gt;% count(word, sort = TRUE, name = &quot;Book_Total&quot;) ) %&gt;% bind_tf_idf(word, Chapter, Chapter_Total) %&gt;% filter(Chapter_Total!=Book_Total) %&gt;% filter(tf&lt;1) %&gt;% arrange(-tf_idf) %&gt;% group_by(Chapter) %&gt;% top_n(4) %&gt;% ungroup() %&gt;% mutate(word = fct_reorder(word, tf_idf)) %&gt;% ggplot(aes(x = word,y = tf_idf, fill = Chapter)) + geom_col(show.legend = FALSE) + facet_wrap(~Chapter, scales = &quot;free&quot;, ncol = 4) + coord_flip() ## Joining, by = &quot;word&quot; ## Selecting by tf_idf 6.10 Topic Modeling Create Document Term Matrix library(topicmodels) bookdtm = booktokens %&gt;% left_join(get_sentiments(&quot;nrc&quot;)) %&gt;% filter(!is.na(sentiment)) %&gt;% select(Chapter,word) %&gt;% count(Chapter,word) %&gt;% rename(document = Chapter, term = word, count = n) %&gt;% mutate(document = as.integer(document), count = as.double(count)) %&gt;% cast_dtm(document, term, count) ## Joining, by = &quot;word&quot; Create a reproducable example of two topics lda &lt;- LDA(bookdtm, k = 2, control = list(seed = 1234)) lda ## A LDA_VEM topic model with 2 topics. Extract Topics and ‘Beta’ of each topic. Beta represents topic-word density. Beta: In each topic, how dense is this word? Higher is more dense. Lower is less dense topics &lt;- tidy(lda, matrix = &quot;beta&quot;) ## Warning: `tbl_df()` is deprecated as of dplyr 1.0.0. ## Please use `tibble::as_tibble()` instead. ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_warnings()` to see where this warning was generated. topics ## # A tibble: 3,526 x 3 ## topic term beta ## &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 advantage 3.10e- 4 ## 2 2 advantage 9.02e-79 ## 3 1 adventurous 1.03e- 4 ## 4 2 adventurous 7.65e-79 ## 5 1 afraid 1.95e- 3 ## 6 2 afraid 2.91e- 3 ## 7 1 arrest 1.03e- 4 ## 8 2 arrest 3.63e-79 ## 9 1 astronomer 4.14e- 4 ## 10 2 astronomer 4.49e-78 ## # ... with 3,516 more rows topics %&gt;% arrange(-beta) ## # A tibble: 3,526 x 3 ## topic term beta ## &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 good 0.0333 ## 2 2 good 0.0279 ## 3 2 found 0.0209 ## 4 2 hope 0.0181 ## 5 1 boy 0.0176 ## 6 2 awful 0.0162 ## 7 1 money 0.0152 ## 8 1 found 0.0119 ## 9 1 aunt 0.0116 ## 10 2 boy 0.0106 ## # ... with 3,516 more rows Top Terms top_terms &lt;- topics %&gt;% group_by(topic) %&gt;% top_n(10, beta) %&gt;% ungroup() %&gt;% arrange(topic, -beta) top_terms ## # A tibble: 20 x 3 ## topic term beta ## &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 good 0.0333 ## 2 1 boy 0.0176 ## 3 1 money 0.0152 ## 4 1 found 0.0119 ## 5 1 aunt 0.0116 ## 6 1 white 0.0105 ## 7 1 time 0.0104 ## 8 1 awful 0.00809 ## 9 1 tree 0.00797 ## 10 1 mother 0.00794 ## 11 2 good 0.0279 ## 12 2 found 0.0209 ## 13 2 hope 0.0181 ## 14 2 awful 0.0162 ## 15 2 boy 0.0106 ## 16 2 time 0.0105 ## 17 2 muff 0.00911 ## 18 2 devil 0.00911 ## 19 2 young 0.00895 ## 20 2 murder 0.00820 top_terms %&gt;% mutate(term = reorder_within(term, beta, topic)) %&gt;% ggplot(aes(term, beta, fill = factor(topic))) + geom_col(show.legend = FALSE) + facet_wrap(~ topic, scales = &quot;free&quot;) + coord_flip() + scale_x_reordered() 6.10.1 Comparison of Use Between Topics beta_spread &lt;- topics %&gt;% mutate(topic = paste0(&quot;topic&quot;, topic)) %&gt;% spread(topic, beta) %&gt;% filter(topic1 &gt; .001 | topic2 &gt; .001) %&gt;% mutate(log_ratio = log2(topic2 / topic1)) beta_spread %&gt;% top_n(10, log_ratio) %&gt;% arrange(-log_ratio) ## # A tibble: 10 x 4 ## term topic1 topic2 log_ratio ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 outburst 4.53e-85 0.00106 270. ## 2 including 9.30e-85 0.00137 270. ## 3 freely 1.25e-84 0.00182 270. ## 4 providing 2.22e-84 0.00243 269. ## 5 fee 2.40e-84 0.00243 269. ## 6 worry 1.29e-84 0.00122 269. ## 7 damages 1.54e-84 0.00122 269. ## 8 agreement 8.21e-84 0.00547 268. ## 9 provide 3.84e-84 0.00213 268. ## 10 information 2.79e-84 0.00122 268. beta_spread %&gt;% top_n(-10, log_ratio) %&gt;% arrange(log_ratio) ## # A tibble: 10 x 4 ## term topic1 topic2 log_ratio ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 labor 0.00259 9.14e-79 -251. ## 2 worship 0.00103 3.53e-78 -247. ## 3 owing 0.00290 1.88e-77 -246. ## 4 cutting 0.00103 7.69e-78 -246. ## 5 highest 0.00124 1.96e-77 -245. ## 6 comrade 0.00166 3.32e-77 -245. ## 7 music 0.00155 6.73e-77 -244. ## 8 grim 0.00124 6.40e-77 -243. ## 9 difficulty 0.00166 1.14e-76 -243. ## 10 indifference 0.00103 1.69e-76 -242. “Gamma”: From the documentation: Each of these values is an estimated proportion of words from that document that are generated from that topic. For example, the model estimates that about 41.7% of the words in document 6 were generated from topic 1. 58.3% of the words in document 6 were generated by topic 2. documents &lt;- tidy(lda, matrix = &quot;gamma&quot;) documents %&gt;% arrange(as.numeric(document)) ## # A tibble: 64 x 3 ## document topic gamma ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1 1 1.00 ## 2 1 2 0.0000677 ## 3 2 1 1.00 ## 4 2 2 0.000114 ## 5 3 1 1.00 ## 6 3 2 0.0000601 ## 7 4 1 1.00 ## 8 4 2 0.0000469 ## 9 5 1 1.00 ## 10 5 2 0.0000702 ## # ... with 54 more rows documents %&gt;% filter(document==6) ## # A tibble: 2 x 3 ## document topic gamma ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 6 1 0.417 ## 2 6 2 0.583 "]
]
