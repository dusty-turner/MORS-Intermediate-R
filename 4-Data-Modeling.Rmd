# Data Modeling

In the last year or so, [Max Kuhn]("https://twitter.com/topepos"), working for RStudio, has developed `tidymodels()'.  Tidymodels is a modeling framework that follows tidy coding principles.  It provides a streamlined technique for preprocessing data, executing a model, and validating the output. 

In this section, you will learn:

1. A small amount of data exploration.
2. The basics of `tidymodels`
 - spliting data
 - runing multiple models
 - cross validation
 - selective the best model
3. How to parallelize your code

In this section, we will use the following libraries and data:

```{r, cache=TRUE}
library(tidyverse)
library(tidymodels)
library(tune)
library(janitor)
library(doFuture)
library(vip)

data <- read_csv("data_sources/Batting.csv", col_types = cols(SF = col_double(), GIDP = col_double())) %>%
  clean_names()

hofdata <- read_csv("data_sources/HallOfFame.csv") 

head(data)

head(hofdata)
```

Lets try to develop an informed answer to the question: What baseball statistics are important for a baseball player making the Hall of Fame?

## Prapare Data For Analysis

```{r, cache=TRUE}

hofdata <-
  hofdata %>% 
  clean_names() %>% 
  select(player_id, inducted) %>% 
  mutate(hof = ifelse(inducted=="Y",1,0)) %>% 
  filter(hof==1)

hofdata %>%  count(inducted)

retiredyear <- 
data %>% 
  group_by(player_id) %>% 
  summarise(lastyear = max(year_id))

retiredyear

hof <-
  data %>% 
  group_by(player_id) %>% 
  summarise_at(vars(g:gidp), list(~sum(.,na.rm = TRUE))) %>%
  ungroup() %>%  
  left_join(retiredyear) %>% 
  left_join(hofdata) %>% 
  mutate(inducted = if_else(inducted=="Y",1,0)) %>% 
  mutate(inducted = replace_na(inducted, 0)) %>% 
  filter(g>=1000) %>% 
  mutate(inducted = as.factor(inducted)) %>% 
  select(-hof)

hof

```

### Filter for HOF Eligable Players {-}

```{r, cache=TRUE}
hof %>% 
  filter(lastyear <= 2012) %>% 
  count(inducted) 

hofmod <-
hof %>% 
  filter(lastyear <= 2012) %>% 
  select(-lastyear)
hofmod

hoftest <-
hof %>% 
  filter(lastyear > 2012) %>% 
  select(-lastyear,-inducted)
hoftest

```

## Data Exploration

Percentage of players in the dataset who are in the HOF

```{r, cache=TRUE}
hofmod %>% count(inducted)
```

Visual difference in statistics between HOF and non HOF

```{r, cache=TRUE}

hof %>% 
  select(g:gidp, inducted) %>%  
  pivot_longer(cols = g:gidp) %>% 
  group_by(inducted,name) %>% 
  summarise(
    lwr_quantile = quantile(value, c(.025)),
    median = quantile(value, c(.5)),
    upper_quantile = quantile(value, c(.975)),
            ) %>% 
  ggplot(aes(x=inducted,y=median)) +
  geom_point() +
  geom_errorbar(aes(ymin = lwr_quantile, ymax = upper_quantile)) +
  theme(legend.position = "none") +
  labs(x= "",y = "", title = "Differences Between HOF and non-HOF") +
  facet_wrap(~name, scales = "free")
```

Well, won't players who are in the HOF had played longer?  Therefore they should have more hits, home runs, etc?

```{r, cache=TRUE}
hof %>% 
  select(g, inducted) %>%  
  ggplot(aes(x=g, fill = inducted)) +
  geom_density() +
  labs(x="Games", y = "", fill = "HOF",
       title = "Density of Game / HOF Stats")
```


Lets look at this for every statistic

```{r, cache=TRUE}
hof %>% 
  select(g:gidp, inducted) %>%  
  pivot_longer(cols = g:gidp) %>% 
  ggplot(aes(x=value, fill = inducted)) +
  geom_density() +
  facet_wrap(~name, scales = "free") +
  labs(x="",y = "", fill = "HOF",
       title = "Density of Stats for Players In/Out of HOF")
  
```


## Split Data test/train

```{r, cache=TRUE}
set.seed(str_length("beatnavy"))
hof_initial_split <- initial_split(hofmod, prop = 0.80)
hof_initial_split
```

## Preprocess Data

A recipe is a description of what steps should be applied to a data set in order to get it ready for data analysis.

`tidymodels` / `recipes` currently offers about 30 'steps'.  More documentation for the `recipes` package is [here]("https://recipes.tidymodels.org/reference/index.html")

```{r, cache=TRUE}
preprocessing_recipe <-
  recipe(inducted ~ ., data = training(hof_initial_split)) %>%
  step_knnimpute(all_numeric()) %>% 
  step_center(all_numeric()) %>% 
  step_scale(all_numeric()) %>% 
  step_rm(player_id) %>%
  prep()
preprocessing_recipe
```

## Apply Preprocessing

For a recipe with at least one preprocessing operations that has been trained by `recipe()`, apply the computations to the data.

```{r, cache=TRUE}
hof_training_preprocessed_tbl <- 
  preprocessing_recipe %>% 
  bake(training(hof_initial_split))

hof_training_preprocessed_tbl
```

## Prepare Cross Validation

This partitions our data into `v` folds.  In our case, 5.

This yields a data frame with a nested list of training / testing data.

```{r, cache=TRUE}
set.seed(str_length("beatnavy"))
hof_cv_folds <-
  training(hof_initial_split) %>% 
    bake(preprocessing_recipe, new_data = .) %>%
    vfold_cv(v = 5)
hof_cv_folds
```

## Specify Models 

Now that we've prepared our data, we must specifiy the models which want to compare.  We'll look at 2.

You must first specify the model type (`logistic_reg()` and `rand_forest` in the examples below.)  There are many to choose from in the parsnip package.  Documentation can be found [here]("tidyverse.org/blog/2018/11/parsnip-0-0-1/").

The motivation behind this package from the documentation, "Modeling functions across different R packages can have very different interfaces. If you would like to try different approaches, there is a lot of syntactical minutiae to remember. The problem worsens when you move in-between platforms.

"`parsnip` tries to solve this by providing similar interfaces to models. For example, if you are fitting a random forest model and would like to adjust the number of trees in the forest there are different argument names to remember depending on the random forest package you chose..." (such as `rf` or `randomforest`).

After you specify the model type, you can provide 

### GLM Model {-}

```{r, cache=TRUE}
glmnet_model <-
  logistic_reg(mode = "classification",
    penalty = tune(),
    mixture = tune()) %>%
  set_engine("glmnet")

glmnet_model
```

### Random Forest Model {-}

Notice how in the previous model and in this model, I did not speciy the parameters such as `penalty`, `mixture`, `mode`, `mtry`, `trees`, and `min_n`.  I can specivy these values, but I have chose to 'tune' them through cross validation in later steps.

```{r, cache=TRUE}
forest_model <-
  rand_forest(
    mode = "classification",
    mtry = tune(),
    trees = tune(),
    min_n = tune()
  ) %>%
  set_engine("randomForest", objective = "reg:squarederror")

forest_model
```

## Create Grid of Parameters to Validate Over

Here we specify the tuning parameters for cross validation and take a look visually at the parameter space we are covering in efforts to reach the best model. 

### GLM Model {-}

```{r, cache=TRUE}
glmnet_params <- parameters(penalty(), mixture())
glmnet_params

set.seed(str_length("beatnavy"))
glmnet_grid = grid_max_entropy(glmnet_params, size = 20)
glmnet_grid

glmnet_grid %>%
  ggplot(aes(penalty, mixture)) +
  geom_point(size = 3) +
  scale_x_log10() +
  labs(title = "Max Entropy Grid", x = "Penalty (log scale)", y = "Mixture")
```

### Random Forest Model {-}

```{r, cache=TRUE}
forest_params <- parameters(mtry(c(2,6)), trees(), min_n())
forest_params

set.seed(str_length("beatnavy"))
forest_grid <- grid_max_entropy(forest_params, size = 30)
forest_grid

```

## Execute Cross Validation

### Parallel Processing

The great thing about cross validation is that it helps tune our parameters in efforts to find the model that performs the best on the out of sample test set.  The problem is that it can be compuationally intensive.  

Thankfully `tidymodels` is capable of running over multiple cores.  You can see from the code below my computer has 4 cores.  Your milage may vary. 

```{r message=FALSE, warning=FALSE, cache=TRUE}
all_cores <- parallel::detectCores(logical = FALSE)

registerDoFuture()
cl <- makeCluster(all_cores)
plan(cluster, workers = cl)
```

### GLM Model {-}

The code below kicks off the cross validation of your model.  You can see the time it took to run the code below.  

```{r message=FALSE, warning=FALSE, cache=TRUE}

tictoc::tic()
glmnet_stage_1_cv_results_tbl <- tune_grid(
          object = glmnet_model, 
          inducted ~ ., 
          resamples = hof_cv_folds, 
          grid = glmnet_grid, 
          # grid = forest_grid, 
          metrics = metric_set(accuracy, kap, roc_auc), 
          control = control_grid(verbose = TRUE)
          )
tictoc::toc()

glmnet_stage_1_cv_results_tbl

glmnet_stage_1_cv_results_tbl %>% show_best("accuracy", n = 10)
glmnet_stage_1_cv_results_tbl %>% show_best("kap", n = 10)
glmnet_stage_1_cv_results_tbl %>% show_best("roc_auc", n = 10)
```

### Random Forest Model {-}

This code took quite a bit of time - but would have been much longer if not parrallelized.

```{r message=FALSE, warning=FALSE, cache=TRUE}

tictoc::tic()
forest_stage_1_cv_results_tbl <- tune_grid(
    formula   = inducted ~ .,
    model     = forest_model,
    resamples = hof_cv_folds,
    grid      = forest_grid,
    metrics   = metric_set(accuracy, kap, roc_auc),
    control   = control_grid(verbose = TRUE)
)
tictoc::toc()

forest_stage_1_cv_results_tbl

forest_stage_1_cv_results_tbl %>% show_best("accuracy", n = 10)
forest_stage_1_cv_results_tbl %>% show_best("kap", n = 10)
forest_stage_1_cv_results_tbl %>% show_best("roc_auc", n = 10)

```

## Select Best Parameters

Now that both models are cross validated, we can select the tuning parameters which minimized our error metric.  

```{r, cache=TRUE}
params_glmnet_best <- 
  glmnet_stage_1_cv_results_tbl %>% 
  select_best("roc_auc")

params_glmnet_best

params_forest_best <-
  forest_stage_1_cv_results_tbl %>% 
  select_best("roc_auc")

params_forest_best
```

## Save Best Paramenters

```{r, cache=TRUE}
glmnet_stage_2_model <-
  glmnet_model %>% 
  finalize_model(parameters = params_glmnet_best)

glmnet_stage_2_model
```

```{r, cache=TRUE}
forest_stage_2_model <-
  forest_model %>% 
  finalize_model(params_forest_best)

forest_stage_2_model
```

## Compare Models

No, we need to compare the models to see which one performs best on our error metric.

```{r, cache=TRUE}
train_processed <-
  training(hof_initial_split) %>% 
  bake(preprocessing_recipe, new_data = .)

test_processed <-
  testing(hof_initial_split) %>% 
  bake(preprocessing_recipe, new_data = .)
    
target_expr <-
  preprocessing_recipe %>% 
    pluck("last_term_info") %>%
    filter(role == "outcome") %>%
    pull(variable) %>%
    sym()

glmnet_stage_2_metrics <-
  glmnet_stage_2_model %>%
  fit(formula = inducted ~ ., data = train_processed) %>%
  predict(new_data = test_processed) %>%
  bind_cols(testing(hof_initial_split)) %>%
  metrics(!! target_expr, .pred_class)

forest_stage_2_metrics <-   
  forest_stage_2_model %>%
  fit(formula = inducted ~ ., data = train_processed) %>%
  predict(new_data = test_processed) %>%
  bind_cols(testing(hof_initial_split)) %>%
  metrics(!! target_expr, .pred_class)

glmnet_stage_2_metrics %>% 
  mutate(mod = "glmnet") %>% 
  bind_rows(
    forest_stage_2_metrics %>% 
      mutate(mod = "forest")
    ) %>% 
  arrange(.metric,-.estimate)

```

Looks like the Random Forest is the better model.

## Run Best Model on All Data

```{r, cache=TRUE}
model_final <-forest_stage_2_model %>%
    fit(inducted ~ . , data = bake(preprocessing_recipe, new_data = hofmod))
```

## Run Model on New Data

```{r, cache=TRUE, include=FALSE}
hoftest %>%
  bake(preprocessing_recipe, new_data = .) %>%
  predict(model_final, new_data = .) %>% 
  bind_cols(hoftest %>% select(player_id)) %>% 
  arrange(desc(.pred_class))
```

## Variable Importance

```{r, cache=TRUE}
vip(model_final) +
    labs(title = "Random Forest Model Importance - HOF Prediction") 
```


[Many Thanks](https://www.r-bloggers.com/product-price-prediction-a-tidy-hyperparameter-tuning-and-cross-validation-tutorial/)

