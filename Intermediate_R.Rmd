---
title: "Intermediate R"
author: "MAJ Dusty Turner"
date: "15 JUN 2020"
output: bookdown::gitbook
# output:
#   bookdown::html_book:
#     theme: united
site: bookdown::bookdown_site
always_allow_html: yes
documentclass: memoir
classoption: oneside
# geometry: margin=.75in
---

```{r setup, include=FALSE, cache = TRUE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

# Class Introduction

Agenda

1. Data Manipulation
2. Data Visulaization
3. Functional Programming
4. Data Modeling
5. Text Analysis

Incorporateing Intermediate R Techniques

## Expectations

In taking this course, I'm assuming you have a working knowledge of the following...

1. Base R and possibly 'tidy' concepts
2. Computer Coding 
3. Statistics
4. Dad Jokes

## Class Introductions

Around the room:

- Where you are from:
- Who you work for:
- How you are involved in Data Science:
    (ie - statistics, coding, application buidling, manager, etc)
- Why you sad in that seat:
- Favorite sports team:

## Instructors Introduction

### MAJ Dusty Turner

Army 

- Combat Engineer
- Platoon Leader / XO / Company Commander
- Geospatial / Sapper / Route Clearance
- Hawaii / White Sands Missile Range / Iraq / Afghanistan

Education

- West Point '07
  - Operations Research, BS
- Missouri University of Science and Technology '12
  - Engineering Management, MS
- THE Ohio State '16
  - Integrated Systems Engineering, MS
  - Applied Statistics, Graduate Minor

Data Science

- R User Since '14
- Catch me on Twitter [`@dtdusty`](http://www.twitter.com/dtdusty)
- <http://dusty-turner.netlify.com/>

### Robert Ward


## Course Intent

- Be interactive
- Ask questions at any point
- Don't let me move too fast (or too slow)
- Run the code with me

## Get Course Documents

[github repo]https://github.com/dusty-turner/MORS-Intermediate-R

SSH:

$git@github.com:dusty-turner/MORS-Intermediate-R.git$

HTTPS:

https://github.com/dusty-turner/MORS-Intermediate-R.git

## Tidy Ecosystem

![](images/tidyverse.png)


![](images/tidymodels.png)

## Prerequisite Packages

```{r}

```


<!--chapter:end:index.Rmd-->

# Data Manipulation

```{r, cache = TRUE}
library(tidyverse)
library(tidymodels)
library(janitor)
```

## Read in Data

```{r, cache=TRUE, cache = TRUE}
read_csv("data_files/Batting.csv")
```

### Fix Read In Errors {-}

```{r, cache = TRUE}
read_csv("data_files/Batting.csv", col_types = cols(SF = col_double(), GIDP = col_double()))
```

```{r, cache = TRUE}
read_csv("data_files/Batting.csv", guess_max = 10000)
```

### Clean Column Names {-}

```{r, cache = TRUE}
read_csv("data_files/Batting.csv", col_types = cols(SF = col_double(), GIDP = col_double())) %>% 
  clean_names()
```

## Analysis with `dplyr`

Who has the highest career slugging percentage?

### Select {-}

```{r, cache = TRUE}
data = read_csv("data_files/Batting.csv", col_types = cols(SF = col_double(), GIDP = col_double())) %>% 
  clean_names()
```


```{r, cache = TRUE}
data %>% 
  select(player_id,year_id,h,x2b,x3b,hr,ab, g)
```

### Group_by {-}

```{r, cache = TRUE}
data %>% 
  select(player_id,year_id,h,x2b,x3b,hr,ab, g) %>% 
  group_by(player_id)
```

### Summarise {-}

```{r, cache = TRUE}
data %>% 
  select(player_id,year_id,h,x2b,x3b,hr,ab, g) %>% 
  group_by(player_id) %>% 
  summarise(h = sum(h), 
            x2b = sum(x2b),
            x3b = sum(x3b),
            hr = sum(hr),
            ab = sum(ab),
            g = sum(g)) 
```

### Filter {-}

```{r, cache = TRUE}
data %>% 
  select(player_id,year_id,h,x2b,x3b,hr,ab, g) %>% 
  group_by(player_id) %>% 
  summarise(h = sum(h), 
            x2b = sum(x2b),
            x3b = sum(x3b),
            hr = sum(hr),
            ab = sum(ab),
            g = sum(g)) %>% 
  filter(g>1000) 
```

### Mutate {-}

```{r, cache = TRUE}
data %>% 
  select(player_id,year_id,h,x2b,x3b,hr,ab, g) %>% 
  group_by(player_id) %>% 
  summarise(h = sum(h), 
            x2b = sum(x2b),
            x3b = sum(x3b),
            hr = sum(hr),
            ab = sum(ab),
            g = sum(g)) %>% 
  filter(g>1000) %>% 
  mutate(slg = (h + x2b + 2*x3b + 3*hr)/ab)
```

### Arrange {-}

```{r, cache = TRUE}
slgdata = 
data %>% 
  select(player_id,year_id,h,x2b,x3b,hr,ab, g) %>% 
  group_by(player_id) %>% 
  summarise(h = sum(h), 
            x2b = sum(x2b),
            x3b = sum(x3b),
            hr = sum(hr),
            ab = sum(ab),
            g = sum(g)) %>% 
  filter(g>1000) %>% 
  mutate(slg = (h + x2b + 2*x3b + 3*hr)/ab) %>% 
  arrange(desc(slg))

slgdata
```

## Joins

### Player ID Data {-}

```{r, cache = TRUE}
read_csv("data_files/Master.csv") 
```

```{r, cache = TRUE}
read_csv("data_files/Master.csv") %>% 
  select(playerID, nameFirst, nameLast) %>% 
  mutate(player = paste(nameFirst,nameLast)) %>% 
  select(-starts_with("name"))
```


```{r, cache = TRUE}
master = 
read_csv("data_files/Master.csv") %>% 
  select(playerID, nameFirst, nameLast) %>% 
  mutate(player = paste(nameFirst,nameLast)) %>% 
  select(-starts_with("name"))
```

### Join the Data {-}

```{r, cache = TRUE}
slgdata %>% 
  left_join(master, by = c("player_id" = "playerID"))
```

```{r, cache = TRUE}
slgname = 
slgdata %>% 
  left_join(master, by = c("player_id" = "playerID"))
```

### Rename Columns {-}

```{r, cache = TRUE}
slgname %>% 
  rename(doubles = x2b, triples = x3b)
```


```{r, cache = TRUE}
slgname = 
slgname %>% 
  rename(doubles = x2b, triples = x3b)
```

### Reorder Columns {-}

```{r, cache = TRUE}
slgname %>% 
  select(player_id, player, everything())
```

```{r, cache = TRUE}
slgname =
slgname %>% 
  select(player_id, player, everything())
```


## Other dplyr Tricks

### Count {-}

```{r, cache = TRUE}
data
```

```{r, cache = TRUE}
data %>% 
  count(player_id)
```

```{r, cache = TRUE}
data %>% 
  count(player_id, sort = TRUE)
```

### Uncount {-}

```{r, cache = TRUE}
slgname %>% 
  uncount(triples)
```

### Summarise {-}

```{r, cache = TRUE}
data %>% 
  select(player_id,year_id,h,x2b,x3b,hr,ab, g) %>% 
  group_by(player_id) %>% 
  summarise(h = sum(h), 
            x2b = sum(x2b),
            x3b = sum(x3b),
            hr = sum(hr),
            ab = sum(ab),
            g = sum(g)) 
```


```{r, cache = TRUE}
data %>% 
  select(player_id,year_id,h,x2b,x3b,hr,ab, g) %>% 
  group_by(player_id) %>% 
  summarise_at(vars(-year_id),~sum(.))
```

<!--chapter:end:1-Data-Manipulation.Rmd-->

# Data Visualization

## Bar Chart

```{r, cache = TRUE}
slgname
```

### Canvas {-}

```{r, cache = TRUE}
slgname %>% 
  ggplot()
```

### Mapping {-}

```{r, cache = TRUE}
slgname %>% 
  ggplot(aes(x=player, y = hr))
```

### Filter Data {-}

```{r, cache = TRUE}
slgname %>% 
  top_n(20, h) %>% 
  ggplot(aes(x=player, y = hr)) 
```

### Plot 'mechanism' {-}

```{r, cache = TRUE}
slgname %>% 
  top_n(20, h) %>% 
  ggplot(aes(x=player, y = hr)) +
  geom_col()
```

### Coordinate Flip {-}

```{r, cache = TRUE}
slgname %>% 
  top_n(20, h) %>% 
  ggplot(aes(x=player, y = hr)) +
  geom_col() +
  coord_flip()
```

### Reorder Factors {-}

```{r, cache = TRUE}
slgname %>% 
  top_n(20, h) %>% 
  ggplot(aes(x=fct_reorder(player,hr), y = hr)) +
  geom_col() +
  coord_flip()
```

### Color Bars {-}

```{r, cache = TRUE}
slgname %>% 
  top_n(20, h) %>% 
  ggplot(aes(x=fct_reorder(player,hr), y = hr, fill = slg)) +
  geom_col() +
  coord_flip()
```

### Update Labels {-}

```{r, cache = TRUE}
slgname %>% 
  top_n(20, h) %>% 
  ggplot(aes(x=fct_reorder(player,hr), y = hr, fill = slg)) +
  geom_col() +
  coord_flip() +
  labs(x = "Player", y = "Home Runs", 
       title = "Top 20 Home Run Hitters", 
       fill = "Slugging Percentage",
       subtitle = "1871-2016", 
       caption = "*Among Players Who've Played at least 1000 games")
```

## Scatter Plot {-}

```{r, cache = TRUE}
scatterdat = 
data %>% 
  filter(lg_id == "AL") %>% 
  mutate(team_id = fct_lump(team_id, n = 12)) %>% 
  filter(team_id != "Other") %>% 
  filter(complete.cases(.)) %>%
  filter(year_id>1990) %>% 
  group_by(team_id,year_id) %>% 
  summarise_at(vars(g:gidp), ~sum(.))  

scatterdat
```

### Canvas {-}

```{r, cache = TRUE}
scatterdat %>% 
  ggplot() 
```

### Mapping {-}

```{r, cache = TRUE}
scatterdat %>% 
  ggplot(aes(x=year_id,y=hr,color = fct_reorder(team_id,-hr)))
```

### Plot 'mechanism': geom_point() {-}

```{r, cache = TRUE}
scatterdat %>% 
  ggplot(aes(x=year_id,y=hr,color = fct_reorder(team_id,-hr))) +
  geom_point() 
```

### Plot 'mechanism': geom_smooth() {-}

```{r, cache = TRUE}
scatterdat %>% 
  ggplot(aes(x=year_id,y=hr)) +
  geom_point(aes(color = fct_reorder(team_id,-hr))) +
  geom_smooth()
```

### Facet Plots {-}

```{r, cache = TRUE}
scatterdat %>% 
  ggplot(aes(x=year_id,y=hr)) +
  geom_point(aes(color = fct_reorder(team_id,-hr))) +
  geom_smooth() +
  facet_wrap(~team_id)
```

### Update Labels {-}

```{r, cache = TRUE}
scatterdat %>% 
  ggplot(aes(x=year_id,y=hr)) +
  geom_point(aes(color = fct_reorder(team_id,-hr))) +
  geom_smooth() +
  facet_wrap(~team_id) +
  labs(title = "Home Run Changes Over Time",
       subtitle = paste("From", min(scatterdat$year_id), "to", max(scatterdat$year_id)),
       color = "Team",
       x = "Season", y = "Season Home Run Total")
```

### Change Smoothing Line {-}

```{r, cache = TRUE}
scatterdat %>% 
  ggplot(aes(x=year_id,y=hr)) +
  geom_point(aes(color = fct_reorder(team_id,-hr))) +
  geom_smooth(method = "lm") +
  facet_wrap(~team_id) +
  labs(title = "Home Run Changes Over Time",
       subtitle = paste("From", min(scatterdat$year_id), "to", max(scatterdat$year_id)),
       color = "Team",
       x = "Season", y = "Season Home Run Total")
```

## Animation

```{r echo=TRUE, message=FALSE, warning=FALSE, cache = TRUE}
library(gganimate)

animdata =
  data %>%
  filter(!is.na(lg_id)) %>%
  group_by(year_id, team_id) %>%
  summarise_at(vars(hr, so, bb, ab), ~ sum(., na.rm = TRUE)) %>%
  ungroup() %>%
  inner_join(data %>%
               select(year_id, team_id, lg_id) %>%
               distinct()) 

animdata
```


```{r, cache = TRUE}
p = animdata %>% 
  ggplot(aes(x = so,y = hr, color = bb,size = ab,group = year_id)) +
  geom_point() +
  facet_wrap(~ lg_id) +
  transition_states(year_id,transition_length = 1,state_length = 30) +
  labs(title = 'Year: {closest_state, cache = TRUE}') +
  enter_fade() +
  exit_fade()

animate(p,nframes = length(unique(data$year_id)) * 2)
```



<!-- ```{r, cache = TRUE} -->
<!-- library(gapminder) -->

<!-- ggplot(gapminder, aes(gdpPercap, lifeExp, size = pop, colour = country)) + -->
<!--   geom_point(alpha = 0.7, show.legend = FALSE) + -->
<!--   scale_colour_manual(values = country_colors) + -->
<!--   scale_size(range = c(2, 12)) + -->
<!--   scale_x_log10() + -->
<!--   facet_wrap(~continent) + -->
<!--   # Here comes the gganimate specific bits -->
<!--   labs(title = 'Year: {frame_time, cache = TRUE}', x = 'GDP per capita', y = 'life expectancy') + -->
<!--   transition_time(year) + -->
<!--   ease_aes('linear') -->

<!-- # file_renderer(dir = ".", prefix = "gganim_plot", overwrite = FALSE) -->
<!-- ``` -->


<!--chapter:end:2-Data-Visualization.Rmd-->

# Functional Programming

## Iteration

```{r, cache = TRUE}
library(tidyverse)
# library(here)
# here()
# hitters <- read_csv("_bookdown_files/data_files/Batting.csv", guess_max = 10000)
hitters <- read_csv("data_files/Batting.csv", guess_max = 10000)
# hitters <- read_csv(here("_bookdown_files/data_files","Batting.csv"), guess_max = 10000)

# hitters <- read_csv("data_files/Batting.csv", guess_max = 10000) %>%
#   janitor::clean_names()
# hitters
```

## An Interesting Question

Who played the most games and hit the most Home Runs in the 90s in Texas?

```{r, cache = TRUE}

hitters %>% 
  filter(yearID %in% 1990:1999) %>% 
  filter(teamID %in% c("HOU","TEX")) %>% 
  select(playerID,teamID, G,HR) %>% 
  group_by(playerID, teamID) %>% 
  summarise_all(.funs = sum) %>% 
  arrange(desc(G,HR))

```

## A More General Question

Who, from any team over any number of years in any subset of tams had the most of any statistical category?

```{r, cache = TRUE}
min_year <- 1990
max_year <- 1999
teamID <- c("HOU","TEX")
category <- c("G", "HR")

hitters %>% 
  filter(yearID %in% min_year:max_year) %>% 
  filter(teamID %in% !!enquo(teamID)) %>% 
  select(playerID, teamID, category) %>% 
  group_by(playerID, teamID) %>% 
  summarise_all(.funs = sum) %>% 
  arrange_at(desc({{ category }}))

var_quo = rlang::parse_quosures(category)

library(dplyr)
library(rlang)

test_func = function(var){
    var_quo = parse_quosure(var)
    mtcars %>%
      select(!!var_quo) %>%
      arrange(!!var_quo)
}

test_func2 = function(var){
  var_quo = enquo(var)
  mtcars %>%
    select(!!var_quo) %>%
    arrange(!!var_quo)
}
```

## Even more Genarally

```{r, cache = TRUE}
min_year <- 1990
max_year <- 1999
teamID <- c("HOU","TEX")
category <- c("G", "HR")

baseball_fun <- function(min_year = 1990,
                         max_year = 1999,
                         teamID = c("HOU","TEX"),
                         category = c("G", "HR")){
hitters %>% 
  filter(yearID %in% min_year:max_year) %>% 
  filter(teamID %in% !!enquo(teamID)) %>% 
  select(playerID,teamID, category) %>% 
  group_by(playerID, teamID) %>% 
  summarise_all(.funs = sum) %>% 
  arrange_at(desc(category))
}

baseball_fun()

```

## Even more Genarally

```{r, cache = TRUE}
hitters$teamID %>% unique() %>% sort()
baseball_fun(min_year = 2000,max_year = 2009, teamID = "NYA") %>% 
  arrange(desc(HR))
```

## PURRR

```{r}
library(purrr)

map(c("TEX","NYA"), ~baseball_fun(teamID = .)) %>% 
  map(~arrange(.x))

```


<!--chapter:end:3-Functional-Programming.Rmd-->

# Data Modeling

## Load Packages

```{r, cache=TRUE, warning=FALSE, message=FALSE}
library(tidymodels)
library(tune)
library(dials)
library(parsnip)
library(rsample)
library(recipes)
library(textrecipes)
library(yardstick)
library(vip)
library(gghighlight)
library(patchwork)
library(tidyverse)
library(tidyquant)
library(knitr)
library(janitor)
```

## Read Data

```{r, cache=TRUE}
data = read_csv("data_files/Batting.csv", col_types = cols(SF = col_double(), GIDP = col_double())) %>%
  clean_names()

hofdata = read_csv("data_files/HallOfFame.csv") %>% 
  clean_names() %>% 
  select(player_id, inducted) %>% 
  mutate(hof = ifelse(inducted=="Y",1,0)) %>% 
  filter(hof==1)

hofdata %>%  count(inducted)

retiredyear = 
data %>% 
  group_by(player_id) %>% 
  summarise(lastyear = max(year_id))
retiredyear

hof =
data %>% 
  group_by(player_id) %>% 
  summarise_at(vars(g:gidp), list(~sum(.,na.rm = TRUE))) %>%
  ungroup() %>%  
  left_join(retiredyear) %>% 
  left_join(hofdata) %>% 
  mutate(inducted = if_else(inducted=="Y",1,0)) %>% 
  mutate(inducted = replace_na(inducted, 0)) %>% 
  filter(g>=1000) %>% 
  mutate(inducted = as.factor(inducted)) %>% 
  select(-hof)

hof
```

### Filter for HOF Eligable Players {-}

```{r, cache=TRUE}
hof %>% 
  filter(lastyear <= 2012) %>% 
  count(inducted) 

hofmod =
hof %>% 
  filter(lastyear <= 2012) %>% 
  select(-lastyear)
hofmod

hoftest = 
hof %>% 
  filter(lastyear > 2012) %>% 
  select(-lastyear,-inducted)
hoftest

```

## Data Exploration

```{r, cache=TRUE}
hofmod %>% 
  select(g:inducted) %>%  
  pivot_longer(cols = g:gidp) %>% 
  ggplot(aes(x=value,y=as.factor(inducted),color = name)) +
  geom_point() +
  theme(legend.position = "none") +
  facet_wrap(~name, scales = "free")
```

## Split Data test/train

```{r, cache=TRUE}
set.seed(as.numeric(as.factor("beatnavy")))
hof_initial_split = initial_split(hofmod, prop = 0.80)
hof_initial_split
```

## Preprocess Data

```{r, cache=TRUE}
preprocessing_recipe =
  recipe(inducted ~ ., data = training(hof_initial_split)) %>%
  step_knnimpute(all_numeric()) %>% 
  step_center(all_numeric()) %>% 
  step_scale(all_numeric()) %>% 
  step_rm(player_id) %>%
  prep()
preprocessing_recipe
```

## Apply Preprocessing

```{r, cache=TRUE}
hof_training_preprocessed_tbl = 
  preprocessing_recipe %>% 
  bake(training(hof_initial_split))

hof_training_preprocessed_tbl
```

## Prepare Cross Validation

```{r, cache=TRUE}
set.seed(as.numeric(as.factor("beatnavy")))
hof_cv_folds =
  training(hof_initial_split) %>% 
    bake(preprocessing_recipe, new_data = .) %>%
    vfold_cv(v = 5)
hof_cv_folds
```

## Specify Models 

### GLM Model {-}

```{r, cache=TRUE}
glmnet_model =
  logistic_reg(mode = "classification",
    penalty = tune(),
    mixture = tune()) %>%
  set_engine("glmnet")

glmnet_model
```

### Random Forest Model {-}

```{r, cache=TRUE}
forest_model =
  rand_forest(
    mode = "classification",
    mtry = tune(),
    trees = tune(),
    min_n = tune()
  ) %>%
  set_engine("randomForest", objective = "reg:squarederror")

forest_model
```

## Create Grid of Parameters to Validate Over

### GLM Model {-}

```{r, cache=TRUE}
glmnet_params = parameters(penalty(), mixture())
glmnet_params

set.seed(as.numeric(as.factor("beatnavy")))
glmnet_grid = grid_max_entropy(glmnet_params, size = 20)
glmnet_grid

glmnet_grid %>%
  ggplot(aes(penalty, mixture)) +
  geom_point(size = 3) +
  scale_x_log10() +
  labs(title = "Max Entropy Grid", x = "Penalty (log scale)", y = "Mixture")
```

### Random Forest Model {-}

```{r, cache=TRUE}
forest_params = parameters(mtry(c(2,6)), trees(), min_n())
forest_params

set.seed(as.numeric(as.factor("beatnavy")))
forest_grid = grid_max_entropy(forest_params, size = 30)
forest_grid

```

## Execute Cross Validation

### Parallel Processing

```{r message=FALSE, warning=FALSE, cache=TRUE}
all_cores <- parallel::detectCores(logical = FALSE)

library(doFuture)
registerDoFuture()
cl <- makeCluster(all_cores)
plan(cluster, workers = cl)
```

### GLM Model {-}


```{r message=FALSE, warning=FALSE, cache=TRUE}

library(tune)

glmnet_stage_1_cv_results_tbl = tune_grid(
    formula   = inducted ~ .,
    model     = glmnet_model,
    resamples = hof_cv_folds,
    grid      = glmnet_grid,
    metrics   = metric_set(accuracy, kap, roc_auc),
    control   = control_grid(verbose = TRUE)
)

glmnet_stage_1_cv_results_tbl %>% show_best("accuracy", n = 10, maximize = FALSE)
glmnet_stage_1_cv_results_tbl %>% show_best("kap", n = 10, maximize = FALSE)
glmnet_stage_1_cv_results_tbl %>% show_best("roc_auc", n = 10, maximize = FALSE)
```

### Random Forest Model {-}

```{r message=FALSE, warning=FALSE, cache=TRUE}
forest_stage_1_cv_results_tbl = tune_grid(
    formula   = inducted ~ .,
    model     = forest_model,
    resamples = hof_cv_folds,
    grid      = forest_grid,
    metrics   = metric_set(accuracy, kap, roc_auc),
    control   = control_grid(verbose = TRUE)
)

forest_stage_1_cv_results_tbl %>% show_best("accuracy", n = 10, maximize = FALSE)
forest_stage_1_cv_results_tbl %>% show_best("kap", n = 10, maximize = FALSE)
forest_stage_1_cv_results_tbl %>% show_best("roc_auc", n = 10, maximize = FALSE)

```

## Select Best Parameters

```{r, cache=TRUE}
params_glmnet_best = glmnet_stage_1_cv_results_tbl %>% 
    select_best("roc_auc", maximize = FALSE)
params_glmnet_best

params_forest_best = forest_stage_1_cv_results_tbl %>% 
    select_best("roc_auc", maximize = FALSE)
params_forest_best
```

## Save Best Paramenters

```{r, cache=TRUE}
glmnet_stage_2_model = glmnet_model %>% 
    finalize_model(parameters = params_glmnet_best)

glmnet_stage_2_model
```

```{r, cache=TRUE}
forest_stage_2_model = forest_model %>% 
    finalize_model(params_forest_best)

forest_stage_2_model
```

## Compare Models

```{r, cache=TRUE}
train_processed = 
  training(hof_initial_split) %>% 
  bake(preprocessing_recipe, new_data = .)

test_processed  = 
  testing(hof_initial_split) %>% 
  bake(preprocessing_recipe, new_data = .)
    
target_expr = 
  preprocessing_recipe %>% 
    pluck("last_term_info") %>%
    filter(role == "outcome") %>%
    pull(variable) %>%
    sym()

glmnet_stage_2_metrics =    
  glmnet_stage_2_model %>%
  fit(formula = inducted ~ ., data = train_processed) %>%
  predict(new_data = test_processed) %>%
  bind_cols(testing(hof_initial_split)) %>%
  metrics(!! target_expr, .pred_class)

forest_stage_2_metrics =    
  forest_stage_2_model %>%
  fit(formula = inducted ~ ., data = train_processed) %>%
  predict(new_data = test_processed) %>%
  bind_cols(testing(hof_initial_split)) %>%
  metrics(!! target_expr, .pred_class)

glmnet_stage_2_metrics %>% 
  mutate(mod = "glmnet") %>% 
  bind_rows(
    forest_stage_2_metrics %>% 
      mutate(mod = "forest")
    ) %>% 
  arrange(.metric,-.estimate)

```

Looks like the Random Forest is the better model.

## Run Best Model on All Data

```{r, cache=TRUE}
model_final = forest_stage_2_model %>%
    fit(inducted ~ . , data = bake(preprocessing_recipe, new_data = hofmod))
```

## Run Model on New Data

```{r, cache=TRUE}
hoftest %>%
  bake(preprocessing_recipe, new_data = .) %>%
  predict(model_final, new_data = .) %>% 
  bind_cols(hoftest %>% select(player_id)) %>% 
  arrange(desc(.pred_class))
```

## Variable Importance

```{r, cache=TRUE}
vip(model_final) +
    labs(title = "Random Forest Model Importance - HOF Prediction") 
```


[Many Thanks](https://www.r-bloggers.com/product-price-prediction-a-tidy-hyperparameter-tuning-and-cross-validation-tutorial/)


<!--chapter:end:4-Data-Modeling.Rmd-->

# Text Analysis

## The Adventures of Tom Sawyer


```{r, cache = TRUE}
library(tidyverse)
library(tidytext)
library(stringi)

book = read_file("data_files//The-Adventures-of-Tom-Sawyer.txt") %>% enframe(name = "Book")
# book = read_file("_bookdown_files/data_files/The-Adventures-of-Tom-Sawyer.txt") %>% enframe(name = "Book")
book
book %>% nchar()
```

## Find Chapter Splits

```{r, cache = TRUE}
book =
book %>% separate_rows(value, sep = "\nCHAPTER") %>%
  slice(-1) %>%
  mutate(value = str_remove_all(string = value, pattern = "\n")) %>%
  mutate(value = str_replace(value, "jpg", "HERE")) %>%
  separate(col = "value", into = c("Chapter", "Text"), sep = "HERE") %>%
  filter(!is.na(Text)) %>% 
  mutate(Chapter = unlist(str_extract_all(Chapter, "[A-Z]+"))) %>%
  mutate(Text = str_replace_all(Text, "[.]"," ")) %>% 
  mutate(Chapter = as.numeric(as.roman(Chapter)))

book

```

## Tokenize the Book

```{r, cache = TRUE}
booktokens = book   %>%
  unnest_tokens(word, Text)
booktokens
```

## Remove 'stop words'

```{r, cache = TRUE}
bookstop = booktokens %>%
  anti_join(stop_words)
bookstop
```

## Join Sentiments

```{r, cache = TRUE}
get_sentiments(lexicon = "afinn")
get_sentiments(lexicon = "bing")
get_sentiments(lexicon = "loughran")
get_sentiments(lexicon = "nrc")
```

```{r, cache = TRUE}
booktokens %>%
  left_join(get_sentiments("bing"))

booktokens %>%
  left_join(get_sentiments("bing")) %>%
  filter(!is.na(sentiment))
```

## Descriptive Text Statistics

```{r, cache = TRUE}
booktokens %>%
  left_join(get_sentiments("bing")) %>%
  filter(!is.na(sentiment)) %>%
  count(Chapter,sentiment)
```

## Visualizations

```{r, cache = TRUE}
booktokens %>%
  left_join(get_sentiments("bing")) %>%
  filter(!is.na(sentiment)) %>%
  count(Chapter,sentiment) %>%
  mutate(n = if_else(sentiment == "negative",n*-1,as.double(n))) %>%
  group_by(Chapter) %>%
  mutate(order = group_indices()) %>% 
  summarise(n = sum(n)) %>%
  mutate(pos = if_else(n>0,"pos","neg")) %>%
  ungroup() %>% 
  ggplot(aes(x=Chapter,y=n,fill = pos, color = pos)) +
  geom_col() +
  scale_fill_manual(values = c("red","green")) +
  scale_color_manual(values = c("black","black")) +
  theme(legend.position="none", axis.text.x = element_text(angle = 90)) +
  labs(y = "Net Positive Words",
       title = "Sentiment Analysis of 'The Adventures of Tom Sawyer'",
       subtitle = "Net Positive Words by Chapter")
```

## N-Gram Analysis

### Uni-Grams

```{r, cache = TRUE}
booktokens %>%
  count(word, sort = TRUE)
```

### Remove Stop Words

```{r, cache = TRUE}
booktokens %>%
  left_join(get_sentiments("bing")) %>%
  filter(!is.na(sentiment)) %>%
  count(word,sentiment, sort = TRUE)
```

### Visualize

```{r, cache = TRUE}
booktokens %>%
  left_join(get_sentiments("bing")) %>%
  filter(!is.na(sentiment)) %>%
  count(word, sentiment, sort = TRUE) %>%
  group_by(sentiment) %>%
  top_n(10) %>%
  ungroup() %>%
  ggplot(aes(x=fct_reorder(word,n), y = n, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free_y") +
  coord_flip() +
  labs(x="Word")
```  

### Bigrams

```{r, cache = TRUE}
bookbitokens = book   %>%
  unnest_tokens(bigram, Text, token = "ngrams", n = 2, n_min = 2)
bookbitokens

bookbitokens %>%
  count(bigram, sort = TRUE)
```

### Remove Stop Words in Bigrams

```{r, cache = TRUE}
bookbitokens %>%
  separate(bigram, c("word1", "word2"), sep = " ")

bigrams = 
bookbitokens %>%
  separate(bigram, c("word1", "word2"), sep = " ") %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word)

bigrams %>% 
  count(word1, word2, sort = TRUE)
```

### Visualize Bigrams

```{r, cache = TRUE}
bigrams %>% 
  unite(col = "bigram", word1,word2, sep = " ") %>% 
  count(bigram, sort = TRUE) %>% 
  top_n(20) %>% 
  ggplot(aes(x=fct_reorder(bigram,n),y = n)) +
  geom_col() +
  coord_flip() +
  labs(x="Bigram",y = "Count", title = "Top Bigrams")

```  

## Term Frequency

Term Frequency: The number of times that a term occurs in the book.

Inverse Document Frequency: $\ln(\frac{Total Number of Documents, cache = TRUE}{Total Number of Documents Containing Specified Word, cache = TRUE})$: Measure of how much information the word provides.

Term Frequency - Inverse Document Frequency: Term Frequency * Inverse Document Frequency

### Build TF-IDF Data

Words By Chapter

```{r, fig.height=25, fig.width=12, cache = TRUE}
booktokens %>%
  count(Chapter, word, sort = TRUE, name = "count") %>%
  add_count(word) %>%
  spread(Chapter, count) %>%
  arrange(desc(n))
```

Word Frequency Per Chapter and Book

```{r, cache = TRUE}
booktokens %>%
  count(Chapter, word, sort = TRUE, name = "Chapter_Total") %>%
  left_join(
    booktokens %>%
      count(word, sort = TRUE, name = "Book_Total")
    )
```

Create TF-IDF

```{, cache = TRUE}
booktokens %>%
  count(Chapter, word, sort = TRUE, name = "Chapter_Total") %>%
  left_join(
    booktokens %>%
      count(word, sort = TRUE, name = "Book_Total")
    ) %>%
  bind_tf_idf(word, Chapter, Chapter_Total) %>%
  filter(Chapter_Total!=Book_Total) %>%
  filter(tf<1) %>%
  arrange(-tf_idf)
```


### Visualize TF-IDF

```{r, cache = TRUE}
booktokens %>%
  count(Chapter, word, sort = TRUE, name = "Chapter_Total") %>%
  left_join(
    booktokens %>%
      count(word, sort = TRUE, name = "Book_Total")
    ) %>%
  bind_tf_idf(word, Chapter, Chapter_Total) %>%
  filter(Chapter_Total!=Book_Total) %>%
  filter(tf<1) %>%
  arrange(-tf_idf) %>%
  group_by(Chapter) %>% top_n(4) %>% ungroup() %>%
  mutate(word = fct_reorder(word, tf_idf)) %>%
  ggplot(aes(x = word,y = tf_idf, fill = Chapter)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~Chapter, scales = "free", ncol = 4) +
  coord_flip()
```

## Topic Modeling

Create Document Term Matrix

```{r, cache = TRUE}
library(topicmodels)

bookdtm =
booktokens %>% 
  left_join(get_sentiments("nrc")) %>% 
  filter(!is.na(sentiment)) %>% 
  select(Chapter,word) %>% 
  count(Chapter,word) %>%
  rename(document = Chapter, term = word, count = n) %>% 
  mutate(document = as.integer(document), count = as.double(count))  %>% 
  cast_dtm(document, term, count)
```

Create a reproducable example of two topics

```{r, cache = TRUE}
lda <- LDA(bookdtm, k = 2, control = list(seed = 1234))
lda
```

Extract Topics and 'Beta' of each topic.

Beta represents topic-word density.

Beta: In each topic, how dense is this word?

Higher is more dense.  Lower is less dense

```{r, cache = TRUE}
topics <- tidy(lda, matrix = "beta")
topics
topics %>% arrange(-beta)
```

Top Terms

```{r, cache = TRUE}
top_terms <- topics %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)
top_terms

top_terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip() +
  scale_x_reordered()
```

### Comparison of Use Between Topics

```{r, cache = TRUE}
beta_spread <- topics %>%
  mutate(topic = paste0("topic", topic)) %>%
  spread(topic, beta) %>%
  filter(topic1 > .001 | topic2 > .001) %>%
  mutate(log_ratio = log2(topic2 / topic1)) 

beta_spread %>% 
  top_n(10, log_ratio) %>% arrange(-log_ratio)

beta_spread %>% 
  top_n(-10, log_ratio) %>% arrange(log_ratio)
```

"Gamma": From the documentation:

Each of these values is an estimated proportion of words from that document that are generated from that topic. 

For example, the model estimates that about 41.7% of the words in document 6 were generated from topic 1.  58.3% of the words in document 6 were generated by topic 2.

```{r, cache = TRUE}
documents <- tidy(lda, matrix = "gamma")
documents %>% arrange(as.numeric(document)) 
documents %>% filter(document==6)
```


<!-- ```{r, cache = TRUE} -->
<!-- booktokens %>%  -->
<!--   left_join(get_sentiments("nrc")) %>%  -->
<!--   filter(!is.na(sentiment)) %>%  -->
<!--   select(Chapter,word) %>%  -->
<!--   count(Chapter,word) %>% -->
<!--   rename(document = Chapter, term = word, count = n) %>%  -->
<!--   mutate(document = as.integer(document), count = as.double(count)) %>% -->
<!--   filter(document == 6) %>% -->
<!--   arrange(desc(count)) -->

<!-- ``` -->


<!--chapter:end:5-Text-Analysis.Rmd-->

