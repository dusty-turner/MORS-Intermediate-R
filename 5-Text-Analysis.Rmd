# Text Analysis

In this section, you will learn.

1. More `dplyr`
2. More `ggplot`
3. The basics of `tidytext`
4. The very basics of `topicmodels`

## The Adventures of Tom Sawyer

```{r, cache = TRUE}
library(tidyverse)
library(tidytext)
library(stringi)
library(topicmodels)

book_raw <- read_file("data_sources/The-Adventures-of-Tom-Sawyer.txt") %>% enframe(name = "Book")

book_raw

book_raw %>% nchar()
```

## Find Chapter Splits

To do the analysis, we need to parse the text.  The purpose of this section is not a lesson text parsing so we'll skip the detail.  But I will disuss it a little in class.

```{r, cache = TRUE}
book <-
book_raw %>% 
  separate_rows(value, sep = "\nCHAPTER") %>%
  slice(-1) %>%
  mutate(value = str_remove_all(string = value, pattern = "\n")) %>%
  mutate(value = str_replace(value, "jpg", "HERE")) %>%
  separate(col = "value", into = c("Chapter", "Text"), sep = "HERE") %>%
  filter(!is.na(Text)) %>% 
  mutate(Chapter = unlist(str_extract_all(Chapter, "[A-Z]+"))) %>%
  mutate(Text = str_replace_all(Text, "[.]"," ")) %>% 
  mutate(Text = str_replace_all(Text, "\r"," ")) %>% 
  mutate(Chapter = as.numeric(as.roman(Chapter)))

book

```

## Tokenize the Book

```{r, cache = TRUE}
booktokens <- 
  book %>%
  unnest_tokens(word, Text)

booktokens
```

## Remove 'stop words'

```{r, cache = TRUE}
bookstop <-
  booktokens %>%
  anti_join(stop_words)

bookstop
```

## Join Sentiments

`tidytext` offers several different sentiment packages.  Let's explore.

```{r, cache = TRUE}
get_sentiments(lexicon = "afinn")
get_sentiments(lexicon = "bing")
get_sentiments(lexicon = "loughran")
get_sentiments(lexicon = "nrc")
```

As you can see, each lexicon offers a slightly different way to explore your text.

```{r, cache = TRUE}
booktokens %>%
  left_join(get_sentiments("bing"))

booktokens %>%
  left_join(get_sentiments("bing")) %>%
  filter(!is.na(sentiment))
```

## Descriptive Text Statistics

```{r, cache = TRUE}
booktokens %>%
  left_join(get_sentiments("bing")) %>%
  filter(!is.na(sentiment)) %>%
  count(Chapter,sentiment)
```

## Visualizations

```{r, cache = TRUE}
booktokens %>%
  left_join(get_sentiments("bing")) %>%
  filter(!is.na(sentiment)) %>%
  count(Chapter,sentiment) %>%
  mutate(n = if_else(sentiment == "negative",n*-1,as.double(n))) %>%
  group_by(Chapter) %>%
  mutate(order = cur_group_id()) %>% ## dplyr 1.1.0
  summarise(n = sum(n)) %>%
  mutate(pos = if_else(n>0,"pos","neg")) %>%
  ungroup() %>% 
  ggplot(aes(x=Chapter,y=n,fill = pos, color = pos)) +
  geom_col() +
  scale_fill_manual(values = c("red","green")) +
  scale_color_manual(values = c("black","black")) +
  theme(legend.position="none", axis.text.x = element_text(angle = 90)) +
  labs(y = "Net Positive Words",
       title = "Sentiment Analysis of 'The Adventures of Tom Sawyer'",
       subtitle = "Net Positive Words by Chapter")
```

## N-Gram Analysis

### Uni-Grams

```{r, cache = TRUE}
booktokens %>%
  count(word, sort = TRUE)
```

### Remove Stop Words

```{r, cache = TRUE}
booktokens %>%
  left_join(get_sentiments("bing")) %>%
  filter(!is.na(sentiment)) %>%
  count(word,sentiment, sort = TRUE)
```

### Visualize

```{r, cache = TRUE}
booktokens %>%
  left_join(get_sentiments("bing")) %>%
  filter(!is.na(sentiment)) %>%
  count(word, sentiment, sort = TRUE) %>%
  group_by(sentiment) %>%
  top_n(10) %>%
  ungroup() %>%
  ggplot(aes(x=fct_reorder(word,n), y = n, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free_y") +
  coord_flip() +
  labs(x="Word")
```  

### Bigrams

```{r, cache = TRUE}
bookbitokens <- book   %>%
  unnest_tokens(bigram, Text, token = "ngrams", n = 2, n_min = 2)
bookbitokens

bookbitokens %>%
  count(bigram, sort = TRUE)
```

### Remove Stop Words in Bigrams

```{r, cache = TRUE}
bookbitokens %>%
  separate(bigram, c("word1", "word2"), sep = " ")

bigrams <- 
bookbitokens %>%
  separate(bigram, c("word1", "word2"), sep = " ") %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word)

bigrams %>% 
  count(word1, word2, sort = TRUE)
```

### Visualize Bigrams

```{r, cache = TRUE}
bigrams %>% 
  unite(col = "bigram", word1,word2, sep = " ") %>% 
  count(bigram, sort = TRUE) %>% 
  top_n(20) %>% 
  ggplot(aes(x=fct_reorder(bigram,n),y = n)) +
  geom_col() +
  coord_flip() +
  labs(x="Bigram",y = "Count", title = "Top Bigrams")

```  

## Term Frequency

Term Frequency: The number of times that a term occurs in the book.

Inverse Document Frequency: $\ln(\frac{Total Number of Documents, cache = TRUE}{Total Number of Documents Containing Specified Word, cache = TRUE})$: Measure of how much information the word provides.

Term Frequency - Inverse Document Frequency: Term Frequency * Inverse Document Frequency

### Build TF-IDF Data

Words By Chapter

```{r, fig.height=25, fig.width=12, cache = TRUE}
booktokens %>%
  count(Chapter, word, sort = TRUE, name = "count") %>%
  add_count(word) %>%
  spread(Chapter, count) %>%
  arrange(desc(n))
```

Word Frequency Per Chapter and Book

```{r, cache = TRUE}
booktokens %>%
  count(Chapter, word, sort = TRUE, name = "Chapter_Total") %>%
  left_join(
    booktokens %>%
      count(word, sort = TRUE, name = "Book_Total")
    )
```

Create TF-IDF

```{r, cache = TRUE}
booktokens %>%
  count(Chapter, word, sort = TRUE, name = "Chapter_Total") %>%
  left_join(
    booktokens %>%
      count(word, sort = TRUE, name = "Book_Total")
    ) %>%
  bind_tf_idf(word, Chapter, Chapter_Total) %>%
  filter(Chapter_Total!=Book_Total) %>%
  filter(tf<1) %>%
  arrange(-tf_idf)
```


### Visualize TF-IDF

```{r, cache = TRUE}
booktokens %>%
  count(Chapter, word, sort = TRUE, name = "Chapter_Total") %>%
  left_join(
    booktokens %>%
      count(word, sort = TRUE, name = "Book_Total")
    ) %>%
  bind_tf_idf(word, Chapter, Chapter_Total) %>%
  filter(Chapter_Total!=Book_Total) %>%
  filter(tf<1) %>%
  arrange(-tf_idf) %>%
  group_by(Chapter) %>% top_n(4) %>% ungroup() %>%
  mutate(word = fct_reorder(word, tf_idf)) %>%
  filter(Chapter <= 12) %>% 
  ggplot(aes(x = word,y = tf_idf, fill = Chapter)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~Chapter, scales = "free", ncol = 4) +
  coord_flip()
```

## Topic Modeling

Create Document Term Matrix

```{r, cache = TRUE}
bookdtm <-
booktokens %>% 
  left_join(get_sentiments("nrc")) %>% 
  filter(!is.na(sentiment)) %>% 
  select(Chapter,word) %>% 
  count(Chapter,word) %>%
  rename(document = Chapter, term = word, count = n) %>% 
  mutate(document = as.integer(document), count = as.double(count))  %>% 
  cast_dtm(document, term, count)
```

Create a reproducable example of two topics

```{r, cache = TRUE}
lda <- LDA(bookdtm, k = 2, control = list(seed = 1234))
lda
```

Extract Topics and 'Beta' of each topic.

Beta represents topic-word density.

Beta: In each topic, how dense is this word?

Higher is more dense.  Lower is less dense

```{r, cache = TRUE}
topics <- tidy(lda, matrix = "beta")
topics
topics %>% arrange(topic,-beta)
```

Top Terms

```{r, cache = TRUE}
top_terms <- topics %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)
top_terms

top_terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip() +
  scale_x_reordered()
```

### Comparison of Use Between Topics

```{r, cache = TRUE}
beta_spread <- topics %>%
  mutate(topic = paste0("topic", topic)) %>%
  spread(topic, beta) %>%
  filter(topic1 > .001 | topic2 > .001) %>%
  mutate(log_ratio = log2(topic2 / topic1)) 

beta_spread %>% 
  top_n(10, log_ratio) %>% arrange(-log_ratio)

beta_spread %>% 
  top_n(-10, log_ratio) %>% arrange(log_ratio)
```

"Gamma": From the documentation:

Each of these values is an estimated proportion of words from that document that are generated from that topic. 

For example, the model estimates that about 41.7% of the words in document 6 were generated from topic 1.  58.3% of the words in document 6 were generated by topic 2.

```{r, cache = TRUE}
documents <- tidy(lda, matrix = "gamma")
documents %>% arrange(as.numeric(document)) 
documents %>% filter(document==6)
```


<!-- ```{r, cache = TRUE} -->
<!-- booktokens %>%  -->
<!--   left_join(get_sentiments("nrc")) %>%  -->
<!--   filter(!is.na(sentiment)) %>%  -->
<!--   select(Chapter,word) %>%  -->
<!--   count(Chapter,word) %>% -->
<!--   rename(document = Chapter, term = word, count = n) %>%  -->
<!--   mutate(document = as.integer(document), count = as.double(count)) %>% -->
<!--   filter(document == 6) %>% -->
<!--   arrange(desc(count)) -->

<!-- ``` -->

